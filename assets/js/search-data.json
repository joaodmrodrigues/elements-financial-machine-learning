{
  
    
        "post0": {
            "title": "Mutual information for feature selection",
            "content": "Motivation . In this article, I will explore information-theoretic quantities (mutual information) to perform feature selection for models on continuous variables, like a sequence of price returns, for instance. Mutual information offers many advantages, including: . Model-independent feature selection: Many feature selection techniques are inherent to a particular class of models. For instance, boruta, gini, or mean impurity decrease use random forests to rank the importance among a set of features. While there is an obvious value in selecting features whose predictive power is better leveraged by a particular class of models, this approach may lead to the drop of informative features if the end model is not of the same class as that used for feature selection. In this context, mutual information offers a complete model-agnostic approach to feature selection. . | Full distribution feature selection: If we are trying to model a continuous variable, most feature selection techniques end up selecting features that do better in predicting the mean of the target variable. If we are trying to forecast the return of a given price series, for instance, it is as important to predict its mean as it is to predict is uncertainty, which will improve our allocation to that particular strategy or forecast. This means that we often may benefit from selecting features that carry information about different aspects of the distribution of our target, and not only its mean. Mutual information does exactly that. Even if we are working with classification models, which output buy and sell signals according to some rules, it is extremely important to have a way to model the confidence on these signals. For instance, Prado11. Marcos Lopez de Prado, Advances in Financial Machine Learning, Wiley (2018)↩ . describes using meta-labelling and the training of a surrogate model to predict the confidence on the buy and sell signals derived from the principal model. The best features for this surrogate model may be different than those of the first model. . | Two different kinds of features . Let&#39;s conduct a controlled experiment using two different scenarios: . Variable $X$ (cause) partially determines the mean of variable $Y$ (effect) | Variable $X$ (cause) partially determines the scale (standard deviation) of variable $Y$ (effect) | For the sake of simplicity, let&#39;s restrict ourselves to normal distributions here. . Let&#39;s define a function that generates these samples: . Note: The entire code shown in this article has been written for the purpose of clarity rather than efficiency . import numpy as np def normalize(sample): return (sample-np.mean(sample))/np.std(sample) def generate_samples(N, dependence_coef, which_param): ### Cause samples_cause = np.random.normal(loc=0.0, scale=1.0, size=N) ### Effect inherent_noise = np.random.normal(loc=0.5, scale=1.0, size=N) param = (1-dependence_coef)*inherent_noise + dependence_coef*samples_cause samples_effect = list() for i in range(0, N): if which_param == 1: # relationship on the mean mean = param[i] scale = 1.0 if which_param == 2: # relationship on the scale mean = np.random.normal(loc=np.random.normal(loc=0, scale=1.0)) scale = np.exp(param[i]) [samples_effect.append(np.random.normal(loc=mean, scale=scale))] return [normalize(samples_cause), normalize(samples_effect)] . . Note that we are allowing a varying level of &quot;causation&quot; between $X$ and $Y$, determined by dependence_coef which varies between 0 (no relation) and 1 (strong relation). Also, strong relation does not imply a perfect correlation between the variables. Note that, we are constantly drawing points from univariate normal distributions, where the parameters of the distribution of $Y$ - either mean or scale - depend on the value of $X$. This ensures a more complex stochastic relation between the two random variables. . Let&#39;s generate these samples, assuming a dependence coefficient of 0.5: . N = 10000 dependence_coef = 0.5 # Relationship on the mean (cause_mean, effect_mean) = generate_samples(N=N, dependence_coef=dependence_coef, which_param=1) # Relationship on the scale (cause_scale, effect_scale) = generate_samples(N=N, dependence_coef=dependence_coef, which_param=2) . . And now let&#39;s plot these samples: . import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True) axes[0].plot(cause_mean, effect_mean, &#39;.&#39;, color=(0.8,0.5,0.5,0.2)) axes[0].set_xlabel(&quot;Cause - X&quot;) axes[0].set_ylabel(&quot;Effect - Y&quot;) axes[0].set_title(&quot;Relation on the mean&quot;) axes[1].plot(cause_scale, effect_scale, &#39;.&#39;, color=(0.8,0.5,0.5,0.2)) axes[1].set_xlabel(&quot;Cause - X&quot;) axes[1].set_ylabel(&quot;Effect - Y&quot;) axes[1].set_title(&quot;Relation on the scale&quot;) _=plt.suptitle(&quot;Dependence coefficient = &quot;+str(dependence_coef)) . . As visible from above, on the left-hand side, $X$ is a good feature to use in predicting the mean of $Y$, but completely unrelated with its variance, which is completely stochastic by construction. On the other hand, on the right-hand side, $X$ is completely unrelated with the mean of $Y$, but it&#39;s moderately related to its scale. . Let&#39;s check other dependence coefficients: . Entropy, Joint Entropy and Mutual Information . Subsubtitle . And some text here . var1 = 5 . . var1 = 5 . . This code cell was not shown, only the output . import matplotlib.pyplot as plt import numpy as np x = np.linspace(1, 100, 100) y = np.random.randn(100) plt.plot(x,y) plt.show() . Callout boxes . . Warning: This is a warning . . Important: This is important . . Tip: This is a tip . . Note: This is a note . . Note: This is a note with a link . This is a footnote 1 1. This is the footnote.↩ .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/information%20theory/mutual%20information/feature%20selection/feature%20importance/2021/02/04/writing-mutal_information_and_feature_selection.html",
            "relUrl": "/information%20theory/mutual%20information/feature%20selection/feature%20importance/2021/02/04/writing-mutal_information_and_feature_selection.html",
            "date": " • Feb 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data scientist and systematic trader on a personal account. . Brief resume: . 2021 - present: Data Scientist @ Argus Media, development of forecast models for commodity markets; . | 2019 - 2020: Research Associate @ Imperial College London, machine learning applications in physics; . | 2014 - 2018: PhD in Physics @ University of Lisbon, machine learning description of turbulence; . | . For more information check the links in the bottom of this page… .",
          "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}