{
  
    
        "post0": {
            "title": "Regression to the mean and the decline of out-of-sample performance",
            "content": "Introduction . Essentially all trading strategies that we may develop, either constructed from fundamental (or not) principles or derived from machine learning models, end up having parameters that ultimately need to be somehow optimized and fixed. The commonplace observation is a performance decline when these models are brought to production (out-of-sample performance). . Besides the typical challenges that only emerge during live trading and that can hardly be estimated during backtesting, like price slippage, market impact, etc, there is a fundamental statistical reason behind the out-of-sample performance decline of optimized models: regression to the mean. In fact, parameter optimization in environments of low signal to noise ratio (SNR) is always accompanied by a certain amount of overfitting and selection bias. . In this short article, I will construct a simplified, yet generic numerical experiment demonstrating this effect. I will fix a random seed, in order to make the results completely reproducible: . random_seed = 42 import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm . The simulation model . Let&#39;s begin by setting up a prototype predictive model: . class PredictiveModel(): def __init__(self, SNR): self.p_hit = 1.0/(np.exp(-SNR)+1) def set_seed(self, seed): self.seed = seed return self def get_model_returns(self, security_returns): rand = np.random.default_rng(self.seed) hit = rand.choice(a = [1,-1], size = security_returns.shape[0], p = [self.p_hit, 1-self.p_hit]) model_returns = np.abs(security_returns)*hit return model_returns def get_performance_statistics(self, security_returns): returns = self.get_model_returns(security_returns = security_returns) stats = dict() stats.update({&quot;Sharpe ratio&quot;: np.round((np.mean(returns)/np.std(returns))*np.sqrt(250), 3)}) return stats . The model above is fully characterized by a given signal to noise ratio (SNR). Usually, SNR is a property of the data and not the model. Here, and without loss of generality, we can assume the data to noise-free and introduce noise into the model predictions instead. . The SNR will determine the probability of the model correctly predicting the sign of the next-bar (day, for instance) price return, denoted $p_{ rm{hit}}$. We assume the relation . $$ p_{ rm{hit}} = frac{1}{e^{- rm{SNR}}+1}, $$ . which is plotted below: . def calculate_p_hit(SnR): return 1.0/(np.exp(-SnR)+1) SNR_vals = np.linspace(0, 5, 1000) p_hit_vals = calculate_p_hit(SNR_vals) fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.plot(SNR_vals, p_hit_vals, color=(1.0,0.4,0.4)) axes.set_xlabel(&quot;SNR: Signal to Noise ratio&quot;) axes.set_ylabel(&quot;Mean hit ratio&quot;) plt.show() . . A PredictiveModel object works by receiving an array of returns (simulating the returns of a security) into the get_model_returns method, which outputs a stochastic realization of model returns, associated with a given SNR. We can also call the method get_performance_statistics directly, which outputs the annualized Sharpe Ratio associated with the model returns. Note that the model is stochastic, in the sense that if we associate different random seeds (via the set_seed method), the model returns will be given by a different random realization, depending only on the probability of hit $p_{ rm{hit}}$ associated with the specified SNR. . Let&#39;s see an example: . We begin by sampling n random numbers from a normal distribution ($ mu=0$, $ sigma= rm{volatility}$), representing the security returns . n = 500 volatility = 0.05 rand = np.random.default_rng(random_seed) r = rand.normal(0, volatility, n) . We now instantiate a PredictiveModel object and simulate the performance of a strategy (model) given a specified SNR: . SNR = 0.2 model = PredictiveModel(SNR = SNR).set_seed(seed = random_seed) model_returns = model.get_model_returns(security_returns = r) stats = model.get_performance_statistics(security_returns=r) stats . {&#39;Sharpe ratio&#39;: 0.779} . We can also plot the simulated portfolio of this particular realization of the strategy, assuming a linear (arithmetic) combination of the model returns: . fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.plot(1+np.cumsum(model_returns), color=(1.0,0.4,0.4)) axes.set_xlabel(&quot;Day&quot;) axes.set_ylabel(&quot;Portfolio Value&quot;) plt.show() . . Regression to the mean: numerical example . Let&#39;s now build our numerical illustration of regression to the mean: . We consider an ensemble of n_models models. Each model is instantiated with an SNR drawn from an exponential distribution of mean SNR_mean. This means that within our ensemble, the models have a varying level of predictability. . In order to handle the stochastic nature of the simulation, let&#39;s define two auxiliary objects that draw integer numbers from a uniform distribution (to be used as seeds): . class Sampler_Seed(): def __init__(self, seed): self.rand = np.random.default_rng(seed) def draw(self): return self.rand.integers(1, 1e10, size=1, dtype=int)[0] sampler_seed = Sampler_Seed(seed=random_seed) . and samples from an exponential distribution (to be used as SNR): . class Sampler_SNR(): def __init__(self, seed): self.rand = np.random.default_rng(seed) def draw(self, mean): return self.rand.exponential(scale=mean, size=1)[0] sampler_SNR = Sampler_SNR(seed=random_seed) . Let&#39;s now instantiate the predictive models, each with a different random seed: . n_models = 100 SNR_mean = 0.05 . models = [PredictiveModel(SNR=sampler_SNR.draw(mean=SNR_mean)).set_seed(seed=sampler_seed.draw()) for i in range(n_models)] . Now we can generate a set of returns representing a given security, and split into training (in-sample) and testing (out-of-sample): . n_train = 500 n_test = 500 volatility = 0.05 . r = rand.normal(0, volatility, n_train+n_test) r_train = r[0:n_train] r_test = r[n_train:(n_train+n_test)] . We now run all models in our in-sample optimization period and calculate the respective Sharpe ratio: . SR_train = [model.get_performance_statistics(security_returns=r_train)[&quot;Sharpe ratio&quot;] for model in models] . The strategy can now be optimized by choosing the model whose respective Sharpe ratio is in a given quantile of the full distribution of Sharpe ratios (from all models). By setting this quantile to 1 we are choosing the model with the highest Sharpe ratio: . quantile = 1.0 . optimal_model = models[np.argmin(np.abs(SR_train-np.quantile(SR_train, q=quantile)))] . Let&#39;s check the performance of the model: . model_returns_train = optimal_model.get_model_returns(security_returns=r_train) optimal_model.get_performance_statistics(security_returns=r_train) . {&#39;Sharpe ratio&#39;: 1.927} . Now that we have found the optimal model by maximing the Sharpe ratio, let&#39;s test the model performance in the out-of-sample portion of the data: . optimal_model.set_seed(seed=sampler_seed.draw()) model_returns_test = optimal_model.get_model_returns(security_returns=r_test) optimal_model.get_performance_statistics(security_returns=r_test) . {&#39;Sharpe ratio&#39;: 0.927} . We can also build both in-sample and out-of-sample portfolios: . portfolio_train = 1+np.cumsum(model_returns_train) portfolio_test = 1+np.cumsum(model_returns_test) . fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.plot(portfolio_train, color=(1.0,0.4,0.4), label=&quot;in-sample&quot;) axes.plot(portfolio_test, color=(0.4,0.4,1.0), label=&quot;out-of-sample&quot;) axes.set_ylim([0.5, np.max(portfolio_train)]) axes.set_xlabel(&quot;Day&quot;) axes.set_ylabel(&quot;Portfolio Value&quot;) axes.legend() plt.show() . . Indeed the out-of-sample performance has greatly declined, despite all the data-generating processes being stationary. The reason behind this completely universal and ubiquitous effect lies in the fact that we are optimizing a model in the presence of noise. The optimal model has been selected partially due to its superior predictive power but also because of mere sample fluctuations of its set of predictions, due to the finite noise. In any optimization procedure, the presence of noise will always lead to some degree of overfitting, which is then responsible for the decline of out-of-sample performance. This effect becomes amplified in regimes of lower signal to noise ratio. . Let&#39;s conduct a more sophisticated version of the above Monte Carlo simulation, where the procedure above is repeated many times, so we can understand the effect beyond the result of a single realization. In fact, the out-of-sample performance decline is expected only on average. . Full Monte Carlo simulation . In this section we are simply going to wrap the above procedure inside a run_experiment function, which will then be called for a large number of realizations: . def run_experiment(n_realizations, n_train, n_test, volatility, n_models, SNR_mean, quantile): # List to collect the in-sample and out-of-sample Sharpe ratio of the optimized strategies in_SR = list() out_SR = list() # Runs the full experiment for i in range(0, n_realizations): realization_seed = sampler_seed.draw() rand = np.random.default_rng(realization_seed) # Draws the securities returns r = rand.normal(0, volatility, n_train+n_test) r_train = r[0:n_train] r_test = r[n_train:(n_train+n_test)] # Instantiates the different models models = [PredictiveModel(SNR=sampler_SNR.draw(mean=SNR_mean)).set_seed(seed=sampler_seed.draw()) for i in range(n_models)] # Calculates the in-sample Sharpe ratio for each model SR = [model.get_performance_statistics(security_returns=r_train)[&quot;Sharpe ratio&quot;] for model in models] # Finds the optimal model optimal_model = models[np.argmin(np.abs(SR-np.quantile(SR, q=quantile)))] # Retrieves the in-sample Sharpe ratio in_stats = optimal_model.get_performance_statistics(security_returns=r_train) in_SR.append(in_stats[&quot;Sharpe ratio&quot;]) # Retrieves the out-of-sample Sharpe ratio out_stats = optimal_model.set_seed(seed=sampler_seed.draw()).get_performance_statistics(security_returns=r_test) out_SR.append(out_stats[&quot;Sharpe ratio&quot;]) return np.array(in_SR), np.array(out_SR) . Let&#39;s now set the parameters of the simulation, . n_realizations = 100 n_train = 500 n_test = 500 volatility = 0.05 n_models = 100 quantile = 1 . and repeat the entire process for several values of the SNR mean: . SNR_mean_vals = np.linspace(0, 0.20, 50) . in_SRs = list() out_SRs = list() for SNR_mean_val in tqdm(SNR_mean_vals): in_SR, out_SR = run_experiment(n_realizations = n_realizations, n_train = n_train, n_test = n_test, volatility = volatility, n_models = n_models, SNR_mean = SNR_mean_val, quantile = quantile) tmp_in = np.zeros((in_SR.shape[0], 2)) tmp_in[:,0] = SNR_mean_val tmp_in[:,1] = in_SR tmp_out = np.zeros((out_SR.shape[0], 2)) tmp_out[:,0] = SNR_mean_val tmp_out[:,1] = out_SR in_SRs.append(tmp_in) out_SRs.append(tmp_out) in_SRs = np.array(in_SRs) out_SRs = np.array(out_SRs) . 100%|██████████| 50/50 [01:23&lt;00:00, 1.66s/it] . Let&#39;s now plot the results: . fig, axes = plt.subplots(1, 1, figsize=(7, 5)) axes.scatter([], [], facecolor=(1.0,0.5,0.5), label=&quot;In-sample&quot;) axes.scatter(x=in_SRs[:,:,0], s=80, y=in_SRs[:,:,1], facecolor=(1.0,0.5,0.5,0.05), edgecolors=&quot;none&quot;) axes.scatter([], [], facecolor=(0.5,0.5,1.0), label=&quot;Out-of-sample&quot;) axes.scatter(x=out_SRs[:,:,0], s=80, y=out_SRs[:,:,1], facecolor=(0.5,0.5,1.0,0.05), edgecolors=&quot;none&quot;) axes.legend() axes.plot(SNR_mean_vals, np.mean(in_SRs[:,:,1],1), color=(0.4,0.4,0.4)) axes.plot(SNR_mean_vals, np.mean(out_SRs[:,:,1],1), color=(0.4,0.4,0.4)) axes.set_xlabel(&quot;Mean SNR&quot;) axes.set_ylabel(&quot;Sharpe ratio&quot;) axes.set_ylim([-2.5, 12]) plt.show() . . Let&#39;s now repeat the simulation but increasing the number of models in the optimization universe to 1000: . n_models = 1000 ### Runs simulation in_SRs = list() out_SRs = list() for SNR_mean_val in tqdm(SNR_mean_vals): in_SR, out_SR = run_experiment(n_realizations = n_realizations, n_train = n_train, n_test = n_test, volatility = volatility, n_models = n_models, SNR_mean = SNR_mean_val, quantile = quantile) tmp_in = np.zeros((in_SR.shape[0], 2)) tmp_in[:,0] = SNR_mean_val tmp_in[:,1] = in_SR tmp_out = np.zeros((out_SR.shape[0], 2)) tmp_out[:,0] = SNR_mean_val tmp_out[:,1] = out_SR in_SRs.append(tmp_in) out_SRs.append(tmp_out) in_SRs = np.array(in_SRs) out_SRs = np.array(out_SRs) ### Plotting fig, axes = plt.subplots(1, 1, figsize=(7, 5)) axes.scatter([], [], facecolor=(1.0,0.5,0.5), label=&quot;In-sample&quot;) axes.scatter(x=in_SRs[:,:,0], s=80, y=in_SRs[:,:,1], facecolor=(1.0,0.5,0.5,0.05), edgecolors=&quot;none&quot;) axes.scatter([], [], facecolor=(0.5,0.5,1.0), label=&quot;Out-of-sample&quot;) axes.scatter(x=out_SRs[:,:,0], s=80, y=out_SRs[:,:,1], facecolor=(0.5,0.5,1.0,0.05), edgecolors=&quot;none&quot;) axes.legend() axes.plot(SNR_mean_vals, np.mean(in_SRs[:,:,1],1), color=(0.4,0.4,0.4)) axes.plot(SNR_mean_vals, np.mean(out_SRs[:,:,1],1), color=(0.4,0.4,0.4)) axes.set_xlabel(&quot;Mean SNR&quot;) axes.set_ylabel(&quot;Sharpe ratio&quot;) axes.set_ylim([-2.5, 12]) plt.show() . . 100%|██████████| 50/50 [15:43&lt;00:00, 18.87s/it] . The plots above clearly illustrate the point of this article. The out-of-sample performance decline becomes stronger is situations of low signal to noise ratio. It also illustrate the danger of conducting multiple testing and/or parameter optimization. Even fully random models will show an apparent good performance if enough trials are conducted. This selection bias is one of the most significant contributions to backtest overfit. . An interesting idea that partially handles this limitation is the use of conveniently deflated Sharpe ratios, which examines the distribution of the maximum Sharpe ratio under multiple testing - (Bailey &amp; De Prado, 2014) and (de Prado, 2020). . It is worth noting that in the simulations conducted here, I have considered an ensemble of independent strategies (models). In reality, we often conduct multiple testing on a universe of highly correlated strategies. However, this distinction will only quantitatively affect the results, with the qualitative observations remaining unchanged. . Bailey, D. H., &amp; De Prado, M. L. (2014). The deflated Sharpe ratio: correcting for selection bias, backtest overfitting, and non-normality. The Journal of Portfolio Management, 40(5), 94–107. | de Prado, M. M. L. (2020). Machine learning for asset managers. Cambridge University Press. | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/regression%20to%20the%20mean/model%20optimization/overfitting/backtest%20overfit/2021/05/25/parameter_optimization_regression_mean.html",
            "relUrl": "/regression%20to%20the%20mean/model%20optimization/overfitting/backtest%20overfit/2021/05/25/parameter_optimization_regression_mean.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Feature clustering",
            "content": "Introduction . Typical clustering algorithms aim at grouping similar observations defined on top of a feature space, where similarity is quantified by a given distance metric. . Clustering can also be conducted on the set of features themselves, these becoming the observations. In a previous article, I have described metrics that allow us to calculate distances between features. Once these metrics are available, feature clustering follows naturally. Here, I will extend the previously described framework of optimal probabilistic clustering - check Part I and Part II - to (optimal probabilistic) feature clustering. . Feature clustering allows us to infer redundant features and conduct feature selection. More specific to the systematic investment process, better portfolios can be constructed once we are able to systematically cluster similar securities (de Prado, 2020). In particular, the Hierarchical Risk Parity (HRP) method is built on top of feature clustering (López de Prado, 2016). . Note: For the sake of reproducibility of the results described in this article, all random processes are conducted using a fixed seed. . random_seed = 41 . Concepts Review . Let&#39;s begin by reviewing some concepts I have previously described, necessary to build our probabilistic feature clustering framework. . Correlation distance . We need to be able to infer similarity (or dissimilarity) between features. Quantities like correlation, mutual information, etc, can be helpful, but they are not true metrics. However, true metrics can be derived from these quantities. In particular, from the correlation coefficient, we can define the correlation distance and, from the mutual information we can define the variation of information. For further details please check my previous article. Here, and for the sake of simplicity, I will only use the one-side correlation distance, defined as: . $$ d_{ vert rho vert} (X,Y) = sqrt{1 - vert rho(X,Y) vert}, $$ . where $ rho(X,Y)$ is the correlation coefficient between feature $X$ and $Y$. All the discussion that follows is completely generic to whatever metric we choose to use. In particular, if the dependencies between our features are nonlinear, variation of information may be a better choice of metric. . While working with true metrics is not strictly necessary to perform feature clustering, correlation distance and variation of information have better properties when it comes to measuring feature similarity. Besides, one of the approaches I will describe here implies directly using the distance between features when running the clustering algorithm. In this case, using true metrics becomes strictly necessary. . Optimal probabilisitic clustering . In order to perform optimal probabilistic clustering, I&#39;m going to use the same framework described in my previous series of articles - Part I and Part II - which builds on top of the fc-means algorithm (Dias, 2019). However, the source code has been modified to allow us to run the algorithm using an arbitrary distance metric - metric_func - which is sent as an argument to the class constructor: . import numpy as np import scipy.linalg class FCM: def __init__(self, metric_func, n_clusters=10, max_iter=50, m=2, error=1e-5, random_state=42): self.u, self.centers = None, None self.n_clusters = n_clusters self.max_iter = max_iter self.m = m self.error = error self.random_state = random_state self.metric_func = metric_func def fit(self, X): N = X.shape[0] C = self.n_clusters centers = [] r = np.random.RandomState(self.random_state) u = r.rand(N,C) u = u / np.tile(u.sum(axis=1)[np.newaxis].T,C) iteration = 0 while iteration &lt; self.max_iter: u2 = u.copy() centers = self.next_centers(X, u) u = self.next_u(X, centers) iteration += 1 # Stopping rule if scipy.linalg.norm(u - u2) &lt; self.error: break self.u = u self.centers = centers return self def next_centers(self, X, u): um = u ** self.m return (X.T @ um / np.sum(um, axis=0)).T def next_u(self, X, centers): return self._predict(X, centers) def _predict(self, X, centers): power = float(2 / (self.m - 1)) temp = self.metric_func(X.T, centers.T) ** power denominator_ = temp.reshape((X.shape[0], 1, -1)).repeat(temp.shape[-1], axis=1) denominator_ = temp[:, :, np.newaxis] / denominator_ return 1 / denominator_.sum(2) def predict(self, X): if len(X.shape) == 1: X = np.expand_dims(X, axis=0) u = self._predict(X, self.centers) return np.argmax(u, axis=-1) . . The metric function - metric_func - is built such that it takes two arrays $A$ and $B$ as arguments, of sizes ($N$, $n_A$) and ($N$, $n_B$), respectively, with $N$ the number of observations and $n_A$ and $n_B$ the number of features in $A$ and $B$, respectively. The output is the distance between the features in $A$ and the features in $B$, thus of size ($n_A$, $n_B$). . We are going to make use of both the Euclidean distance (Minkowski distance with parameter $p=2$, but kept here the possibility of other values of $p$), . from scipy.spatial.distance import cdist def minkowski_distance(A, B): return cdist(A.T, B.T, metric=&quot;minkowski&quot;, p=2) . and the one-sided correlation distance as defined above, . def correlation_distance(A, B): N = A.shape[0] cov = (A.T@B)/N - np.tensordot(np.mean(A,0), np.mean(B,0), axes=0) corr = cov / np.tensordot(np.std(A,0), np.std(B,0), axes=0)-1e-9 dist = np.sqrt(1-np.abs(corr)) return dist . In the following, we are going to use some of the functions I have described in my previous article on optimal probabilistic clustering. Refer to that for further details. Here, the only difference is a ability to use the user-defined distance metric - metric_func: . def run_cluster(metric_func, n_clusters, data, seed=42): # membership probabilities model = FCM(metric_func=metric_func, n_clusters=n_clusters, random_state=seed) model = model.fit(data) p = model.u centers = model.centers # representative cluster representative_cluster = np.argmax(p, 1) # membership entropy Sx = -np.sum(p*np.log(p), 1) / np.log(n_clusters) # total membership entropy (across the entire feature space) S = np.sum(Sx) return centers, p, representative_cluster, Sx, S . . def minimize_membership_entropy(dataset, metric_func, max_clusters=10, lamb=0, seed=42, n_seeds=1): # The number of clusters to try n_clusters_trials = np.arange(2, max_clusters+1, 1) # Creates the seeds if n_seeds == 1: seeds = np.array([seed]) else: rand = np.random.RandomState(seed=seed) seeds = rand.randint(low=1, high=1e9, size=n_seeds) # Runs the clustering for different seeds and number of clusters total_entropies = list() for seed in seeds: total_entropies_ = list() for trial in n_clusters_trials: _, _, _, _, total_entropy = run_cluster(metric_func = metric_func, n_clusters = trial, data = dataset, seed = seed) total_entropies_.append(total_entropy+lamb*trial) total_entropies.append(total_entropies_) total_entropies = np.array(total_entropies) # Finds the optimal number of clusters and the respective seed location = np.argwhere(total_entropies == np.min(total_entropies))[0] optimal_seed = seeds[location[0]] optimal_nclusters = n_clusters_trials[location[1]] return optimal_seed, optimal_nclusters, total_entropies . . Experimental results . Synthetic datasets . Let&#39;s now create synthetic datasets to assess the performance of the feature clustering framework. We want to uniformly sample $(n,k,m)$ partitions, with $n$ the total number of features, $k$ the number of clusters, and $m$ the minimum number of features per cluster. I&#39;m going to use the same approach as in a previous article: . def construct_random_partition(n, k, m, seed=None): rand = np.random.RandomState(seed=seed) parts = rand.choice(range(1, n-k*(m-1)), k-1, replace=False) parts.sort() parts = np.append(parts, n-k*(m-1)) parts = np.append(parts[0], np.diff(parts)) - 1 + m return parts . As a example: . construct_random_partition(n=10, k=4, m=2, seed=random_seed) . array([3, 2, 3, 2]) . Once a $(n,k,m)$ partition has been sampled, we need to draw the observations of each feature. For each cluster $i$, we draw a base sample of n_observations from a standard normal distribution. Each feature belonging to cluster $i$ is populated by summing to the base sample another set of n_observations from a normal distribution of standard deviation sigma. The larger the sigma, the larger the intra-cluster distance between features. Below is the function that implements this structure: . from scipy.stats import norm def generate_realization(n, k, m, n_observations, sigma, seed=None): partition = construct_random_partition(n=n, k=k, m=m, seed=seed) rand = np.random.RandomState(seed=seed) base_samples = rand.normal(0, 1, (n_observations, len(partition))) dataset = sigma*rand.normal(0, 1, (n_observations, n)) j = 0 for h, value in enumerate(partition): base_sample = base_samples[:,h] for i in range(0, value): dataset[:,j] += base_sample j+=1 return dataset, partition . While the above process only allows for controlling the intra-cluster feature similarity, with inter-cluster features being uncorrelated on average, the same could be easily extended to allow a varying degree of dependence between features of different clusters. . In the experiments that follow, we are going to consider the following set of parameters: . n = 200 k = 8 m = 5 n_observations = 1000 sigma = 0.4 dataset, partition = generate_realization(n = n, k = k, m = m, n_observations = n_observations, sigma = sigma, seed = random_seed) . Let&#39;s now calculate both the correlation coefficient and the correlation distance between all the features in our partition: . import pandas as pd correlation_matrix = pd.DataFrame(dataset).corr() # Correlation distance correlation_distance_matrix = correlation_distance(A=dataset, B=dataset) . And let&#39;s plot the results: . import matplotlib.pyplot as plt fig = plt.figure(figsize=(10, 4)) axes0 = plt.axes([0.05, 0.05, 0.40, 0.80]) plot0 = axes0.pcolor(correlation_matrix, cmap=&quot;coolwarm&quot;, vmin=-1, vmax=1) axes0.set_xlabel(&quot;Feature&quot;) axes0.set_ylabel(&quot;Feature&quot;) cbar0 = fig.colorbar(plot0) cbar0.set_label(&#39;Correlation coefficient&#39;, rotation=90) axes1 = plt.axes([0.55, 0.05, 0.40, 0.80]) plot1 = axes1.pcolor(correlation_distance_matrix, cmap=&quot;bone&quot;, vmin=0, vmax=1) axes1.set_xlabel(&quot;Feature&quot;) axes1.set_ylabel(&quot;Feature&quot;) cbar1 = fig.colorbar(plot1) cbar1.set_label(&#39;Correlation Distance&#39;, rotation=90) plt.show() . . For later comparison, let&#39;s create labels to map each feature to its respective cluster. This will be the ground truth we want the optimal clustering algorithm to recover: . cluter_assignment_true = np.array([i for i in range(0, len(partition)) for j in range(0, partition[i])]) . Note: In a more sophisticated treatment of the problem, it may be a good idea to denoise the correlation matrix and, consequently, the distance matrix, before starting the clustering algorithm. There are a few ways this can be pursued. In the context of Principal Component Analysis (PCA), components associated with smaller eigenvalues may be associated with noise and hence removed from the reconstruction of the original dataset. Another technique involves using random matrix theory to infer the eigenmodes associated with noise. This is nicely described by (de Prado, 2020). Here, for the sake of simplicity, I&#39;m going to ignore this step and proceed with two possible approaches to feature clustering. . Feature clustering - based on observation matrix . The first approach I&#39;m describing for feature clustering involves building an observation matrix, of size $n times n$, with $n$ the total number of features as before, whose entries quantify some sort of similarity (or dissimilarity) between features. While simple correlation is a valid option, correlation distance has properties that may be deemed more desirable - check, for instance, (de Prado, 2020). Other possible options include: . Mutual information | Mutual information score - check previous article | Variation of information - check previous article | Two-sided correlation distance - check previous article | etc | . Here, I will use the one-sided correlation distance. The observation matrix is then the correlation distance matrix calculated above: . observation_matrix = correlation_distance_matrix . Let&#39;s now find the optimal number of clusters by minimizing the total membership entropy. The clustering algorithm is set to use the Euclidean distance to calculate the distance between observations of the observation matrix, which correspond to the distance between the features themselves. . max_clusters = 10 optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset = observation_matrix, metric_func = minkowski_distance, max_clusters = max_clusters, lamb = 0, seed = random_seed, n_seeds = 50) print(&quot;Optimal number of clusters =&quot;, optimal_nclusters) . Optimal number of clusters = 8 . Let&#39;s also plot the full entropy minimization results: . fig, axes = plt.subplots(1, 1, figsize=(5, 4)) axes.plot([optimal_nclusters, optimal_nclusters], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) for i in range(0, total_entropies.shape[0]): axes.plot(np.arange(2, max_clusters+1, 1), total_entropies[i,:], color=(0.46,0.46,0.46,0.2), linewidth=2) axes.plot(np.arange(2, max_clusters+1, 1), np.min(total_entropies,0), color=(0.5,0.5,1.0)) axes.set_xlabel(&#39;Number of clusters&#39;) axes.set_ylabel(&#39;Total membership entropy&#39;) plt.tight_layout() plt.show() . . The optimal clustering algorithm correctly predicts the number of clusters. Let&#39;s now fix it and rerun the final clustering using the optimal seed: . centers, p, representative_cluster, Sx, S = run_cluster(metric_func = minkowski_distance, n_clusters = optimal_nclusters, data = observation_matrix, seed = optimal_seed) print(&quot;Total membership entropy =&quot;, np.round(S, 2)) . Total membership entropy = 25.2 . While we have correctly inferred the number of clusters, we are now interested in checking if the cluster assignment is correct. For that purpose, let&#39;s define the $n times n$ affinity matrix $Z$ such that $Z_{ij} in lbrace mathrm{null}, 0, 1, ... , k^* rbrace$ and $Z_{ij} =z$ if and only if $i$ and $j$ both belong to cluster $z$, otherwise if $i$ and $j$ belong to different clusters $Z_{ij} = mathrm{null}$. . Let&#39;s calculate both the actual affinity matrix (ground truth), and that retrieved from the optimal clustering algorithm, using the representative clusters of each observation (feature): . aux2 = np.repeat([cluter_assignment_true], cluter_assignment_true.shape[0], axis=0) affinity_real = aux2==aux2.T Z_real = np.array(affinity_real, dtype=float) Z_real[affinity_real==False] = np.nan for i in range(0, affinity_real.shape[1]): inds = np.where(affinity_real[i,:]==True)[0] Z_real[i, inds] = cluter_assignment_true[i] ### Affinity matrix - retrieved from the optimal clustering framework aux1 = np.repeat([representative_cluster], representative_cluster.shape[0], axis=0) affinity_clustering = aux1==aux1.T Z_clustering = np.array(affinity_clustering, dtype=float) Z_clustering[affinity_clustering==False] = np.nan for i in range(0, affinity_real.shape[1]): inds = np.where(affinity_clustering[i,:]==True)[0] Z_clustering[i, inds] = representative_cluster[i] . Let&#39;s now plot the affinity matrices: . ### Handles the colors from matplotlib import cm import matplotlib def make_rgb_transparent(rgb, alpha): bg_rgb = [1, 1, 1] return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)] colormap = cm.get_cmap(&#39;Accent&#39;) facecolors = list() for i in range(0, optimal_nclusters): facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=0.65)) color_seq = list() for j in range(0, n): color_seq.append(make_rgb_transparent(facecolors[representative_cluster[j]], 1-Sx[j])) pcluster = np.zeros((n,n,3)) for i in range(0, n): inds = np.where(affinity_clustering[i,:]==True)[0] pcluster[i,inds,:] = color_seq[i] pcluster[affinity_clustering==False, :] = 1 ### Plot fig, axes = plt.subplots(1, 3, figsize=(14, 4)) axes[0].pcolor(Z_real, cmap=&quot;Accent&quot;) axes[0].title.set_text(&#39;Ground truth &#39;) axes[0].set_xlabel(&quot;Feature&quot;) axes[0].set_ylabel(&quot;Feature&quot;) axes[1].pcolor(Z_clustering, cmap=&quot;Accent&quot;) axes[1].title.set_text(&#39;Optimal clustering&#39;) axes[1].set_xlabel(&quot;Feature&quot;) axes[1].set_ylabel(&quot;Feature&quot;) axes[2].imshow(pcluster, origin=&#39;lower&#39;) axes[2].title.set_text(&#39;Optimal clustering - entropy mask&#39;) axes[2].set_xlabel(&quot;Feature&quot;) axes[2].set_ylabel(&quot;Feature&quot;) plt.show() . . From the above, we can see that the optimal clustering framework fully recovers the ground truth. Note that, the absolute cluster labels don&#39;t carry any meaning, so the difference in colours between the left and the middle graphs bears no meaning. The right-most plot is a repetition of the middle one, but where a transparency level proportional to the membership entropy of each feature has been added (across the vertical direction). . Feature clustering - based on correlation distance directly . We are now going to circumvent the construction of the observation matrix by running the fc-means algorithm directly on top of the original dataset and using the correlation distance as the distance metric. Now distances will be calculated between features, rather than between observations. Everything else follows in the same way as we have done above, starting by finding the optimal number of clusters by minimizing the total membership entropy: . max_clusters = 10 optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset = dataset.T, metric_func = correlation_distance, max_clusters = max_clusters, lamb = 0, seed = random_seed, n_seeds = 50) print(&quot;Optimal number of clusters =&quot;, optimal_nclusters) . Optimal number of clusters = 5 . fig, axes = plt.subplots(1, 1, figsize=(5, 4)) axes.plot([optimal_nclusters, optimal_nclusters], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) for i in range(0, total_entropies.shape[0]): axes.plot(np.arange(2, max_clusters+1, 1), total_entropies[i,:], color=(0.46,0.46,0.46,0.2), linewidth=2) axes.plot(np.arange(2, max_clusters+1, 1), np.min(total_entropies,0), color=(0.5,0.5,1.0)) axes.set_xlabel(&#39;Number of clusters&#39;) axes.set_ylabel(&#39;Total membership entropy&#39;) axes.set_ylim(80, 160) plt.tight_layout() plt.show() . . Contrary to the previous approach, here we don&#39;t seem to be able to correctly infer the number of clusters. Let&#39;s run the final cluster considering 5 different clusters: . centers, p, representative_cluster, Sx, S = run_cluster(metric_func=correlation_distance, n_clusters=optimal_nclusters, data=dataset.T, seed=optimal_seed) print(&quot;Total membership entropy =&quot;, np.round(S, 2)) . Total membership entropy = 121.46 . Let&#39;s now calculate the affinity matrices, as before: . ### Affinity matrix - real aux2 = np.repeat([cluter_assignment_true], cluter_assignment_true.shape[0], axis=0) affinity_real = aux2==aux2.T Z_real = np.array(affinity_real, dtype=float) Z_real[affinity_real==False] = np.nan for i in range(0, affinity_real.shape[1]): inds = np.where(affinity_real[i,:]==True)[0] Z_real[i, inds] = cluter_assignment_true[i] ### Affinity matrix - retrieved from the optimal clustering framework aux1 = np.repeat([representative_cluster], representative_cluster.shape[0], axis=0) affinity_clustering = aux1==aux1.T Z_clustering = np.array(affinity_clustering, dtype=float) Z_clustering[affinity_clustering==False] = np.nan for i in range(0, affinity_real.shape[1]): inds = np.where(affinity_clustering[i,:]==True)[0] Z_clustering[i, inds] = representative_cluster[i] . . And plot the results: . ### Handles the colors facecolors = list() for i in range(0, optimal_nclusters): facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=0.65)) color_seq = list() for j in range(0, n): color_seq.append(make_rgb_transparent(facecolors[representative_cluster[j]], 1-Sx[j])) pcluster = np.zeros((n,n,3)) for i in range(0, n): inds = np.where(affinity_clustering[i,:]==True)[0] pcluster[i,inds,:] = color_seq[i] pcluster[affinity_real==False, :] = 1 ### Plot fig, axes = plt.subplots(1, 3, figsize=(14, 4)) axes[0].pcolor(Z_real, cmap=&quot;Accent&quot;) axes[0].title.set_text(&#39;Ground truth &#39;) axes[0].set_xlabel(&quot;Feature&quot;) axes[0].set_ylabel(&quot;Feature&quot;) axes[1].pcolor(Z_clustering, cmap=&quot;Accent&quot;) axes[1].title.set_text(&#39;Optimal clustering&#39;) axes[1].set_xlabel(&quot;Feature&quot;) axes[1].set_ylabel(&quot;Feature&quot;) axes[2].imshow(pcluster, origin=&#39;lower&#39;) axes[2].title.set_text(&#39;Optimal clustering - entropy mask&#39;) axes[2].set_xlabel(&quot;Feature&quot;) axes[2].set_ylabel(&quot;Feature&quot;) plt.show() . . Here, we can see that the framework fails to recognize the smaller clusters. Interestingly, however, from the right-most graph, we can see that the features that are mislabelled show maximum entropy, becoming fully transparent. This means that, for this subset of mislabelled features, the representative cluster bears no meaning, since the algorithm essentially fails at associating them to any of the available clusters with high confidence, which is consistent with the known ground truth. Even more important, the features that show smaller membership entropy do get correctly clustered together. . Conclusions . From the analysis above, using the approach based on the observation matrix seems to be more effective for feature clustering. Indeed the observation matrix, by calculating the distance between each feature and all the remaining features, may be more resilient to noise. However, the number of observations is always equal to the number of features - remember that the observation matrix has size $n times n$, with $n$ the number of features. So as the number of features increases, the dimensionality increases. Euclidean distance is known to be essentially meaningless when the number of dimensions is high. A proper assessment of each strategy will involve doing rigorous experiments with varying parameters and conditions. . References: . de Prado, M. M. L. (2020). Machine learning for asset managers. Cambridge University Press. | López de Prado, M. (2016). Building Diversified Portfolios that Outperform Out of Sample. The Journal of Portfolio Management, 42(4), 59–69. https://doi.org/10.3905/jpm.2016.42.4.059 | Dias, M. L. D. (2019). fuzzy-c-means: An implementation of Fuzzy C-means clustering algorithm. Zenodo. https://doi.org/10.5281/zenodo.3066222 | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/feature%20clustering/clustering/correlation%20distance/2021/04/11/feature_clustering.html",
            "relUrl": "/feature%20clustering/clustering/correlation%20distance/2021/04/11/feature_clustering.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Optimal probabilistic clustering - Part II",
            "content": "Introduction . In Part I of this series, I introduced the idea of membership entropy which, in the context of probabilistic clustering models allows us to construct optimal clusters. I have argued that an optimal set of (fuzzy) clusters is that that minimizes the total membership entropy across our data space. In fact, the purpose of clustering is to systematize relations, or proximities, such that intra-cluster variability is minimized, while inter-cluster variability is maximized. This means less information (number of bits, for instance) is needed when we refer to our data in terms of clusters rather than in its unclustered form. Doing this optimally means minimizing this quantity of information needed to describe our data, which leads to the idea of minimizing the total membership entropy. . In Part I I have illustrated the power of this idea using very simple synthetic datasets. In this second part, I will describe some improvements in the framework needed to properly handle more complex and realistic datasets. . I will finish with some systematic experiments to assess the overall performance of the framework. . I have previously defined the membership entropy $S_x$ and the representative cluster, both derived from the set of membership probabilities $p_x(i)$. Here, $p_x(i)$ is defined as the probability of observation $x$ belonging to cluster $i$. Check the previous post for further details: . from fcmeans import FCM def run_cluster(n_clusters, data, seed=42): # membership probabilities model = FCM(n_clusters=n_clusters, random_state=seed) model = model.fit(data) p = model.u centers = model.centers # representative cluster representative_cluster = np.argmax(p, 1) # membership entropy Sx = -np.sum(p*np.log(p), 1) / np.log(n_clusters) # total membership entropy (across the entire feature space) S = np.sum(Sx) return centers, p, representative_cluster, Sx, S . Membership entropy regularization . Regularization is a central concept in machine learning. It allows us to mitigate overfitting by penalizing models with high degrees of freedom. We can think of clustering as the problem of being able to describe a dataset not in terms of individual observations but rather in terms of collective clusters. The number of clusters can then be interpreted as the number of degrees of freedom. We may be interested in promoting a low number of clusters to further reduce the complexity of our data. Models with a higher number of clusters should then be penalized. . I have previously defined the membership entropy as . $$ S_{x}( { p_x(i) }) = - frac{1}{ mathrm{log}(k)} displaystyle sum_{i=1}^k p_x(i) mathrm{log}(p_x(i)), $$with $k$ the number of clusters. In this context, and while we typically find a local entropy minimum at a given value of $k$ - defined as the optimal number of clusters - note that a model where the number of clusters is equal to the total number of observations has zero entropy (each observation is its own cluster, with probability one). In order to promote lower complexity, we can then define the penalized membership entropy . $$ S_x^{ lambda} = S_x + lambda k, $$with $ lambda geq 0$ the regularization parameter. The minimization of $S_x^{ lambda}$ to find the optimal number of clusters $k$ will, to a degree quantified by $ lambda$, promote models with lower complexity. This may be particularly important for datasets with a small number of observations and/or a high number of clusters. . Experimental results . Random datasets . In order to conduct systematic experiments, we need to be able to generate random datasets with bespoke properties. This can be done in a two-step process: . 1) Random partitions: . In order to conduct systematic experiments we need to be able to generate random datasets with bespoke properties. In particular, we want to uniformly sample $(n,k,m)$ partitions, where $n$ is the total number of observations, $k$ is the number of clusters, and $m$ is the minimum number of observations per cluster. The following code, borrowed from (de Prado, 2020) uniformly samples such partitions: . import numpy as np def construct_random_partition(n, k, m, seed=None): rand = np.random.RandomState(seed=seed) parts = rand.choice(range(1, n-k*(m-1)), k-1, replace=False) parts.sort() parts = np.append(parts, n-k*(m-1)) parts = np.append(parts[0], np.diff(parts))-1+m return parts . As an example, let&#39;s sample a partition with $(n=500, k=7, m=2)$. Note: Every function involving random sampling takes a user-defined seed parameter, to allow the reproducibility of the results I show here. . partition = construct_random_partition(n=500, k=7, m=2, seed=40) print(partition) . [153 75 13 33 7 36 183] . 2) Random observations: . After the partition has been generated, we now need to sample random observations. Let&#39;s define a function that takes the previously generated partition as input, together with n_features (the dimensionality of our data space), and std (the width of our clusters). The coordinates of the cluster centers are uniformly sampled from the $[-1, 1]$ interval. . def generate_random_dataset(partition, n_features, std, seed=41): random_state = np.random.RandomState(seed=seed) centers = list() dataset = list() for n in partition: # cluster center coordinates cluster_center = random_state.uniform(-1, 1, n_features) centers.append(cluster_center) # observation coordinates for observation in range(0, n): dataset.append(cluster_center+std*random_state.standard_normal(n_features)) dataset = np.array(dataset) # shuffles the observations dataset = dataset[random_state.permutation(dataset.shape[0]), :] return np.array(dataset), np.array(centers) . Let&#39;s generate and plot a random dataset: . dataset, _ = generate_random_dataset(partition=partition, n_features=2, std=0.04, seed=41) . import matplotlib from matplotlib import cm import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 1, figsize=(5, 4)) axes.scatter(dataset[:,0], dataset[:,1], marker=&#39;.&#39;, s=60, color=(0.5,0.5,0.8,0.5)) axes.set_xlabel(&#39;feature 1&#39;) axes.set_ylabel(&#39;feature 2&#39;) axes.set_xlim(-1.2,1.2) axes.set_ylim(-1.2,1.2) plt.tight_layout() plt.show() . . Dealing with complex datasets . Non-convex optimization . As I have described in Part I, the fc-means algorithm involves the minimization of a particular loss function (Bezdek, 2013). This minimization, however, is not convex and there is the risk of being stuck at a local minimum. While this may not be problematic in simple cases, as the complexity of our dataset increases this becomes a rather serious problem. While a global minimization is always hard to guarantee, we can increase the confidence in the performance by running the fc-means algorithm with different seeds. . Here, we will minimize the total membership entropy like I have described in Part I, but running the algorithm with a total of n_seeds different seeds. Moreover, max_clusters is the maximum number of clusters to consider, and lamb is the regularization parameters introduced above. . def minimize_membership_entropy(dataset, max_clusters=10, lamb=0, seed=42, n_seeds=1): # The number of clusters to try n_clusters_trials = np.arange(2, max_clusters, 1) # Creates the seeds if n_seeds == 1: seeds = np.array([seed]) else: rand = np.random.RandomState(seed=seed) seeds = rand.randint(low=1, high=1e9, size=n_seeds) # Runs the clustering for different seeds and number of clusters total_entropies = list() for seed in seeds: total_entropies_ = list() for trial in n_clusters_trials: _, _, _, _, total_entropy = run_cluster(n_clusters=trial, data=dataset, seed=seed) total_entropies_.append(total_entropy+lamb*trial) total_entropies.append(total_entropies_) total_entropies = np.array(total_entropies) # Finds the optimal number of clusters and the respective seed location = np.argwhere(total_entropies == np.min(total_entropies))[0] optimal_seed = seeds[location[0]] optimal_nclusters = n_clusters_trials[location[1]] return optimal_seed, optimal_nclusters, total_entropies . . Let&#39;s run the minimization process for the dataset generated above: . max_clusters = 12 optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset, max_clusters=max_clusters, lamb=1, n_seeds=100) print(&quot;Optimal number of clusters =&quot;, optimal_nclusters) . Optimal number of clusters = 7 . We have correctly inferred the total number of clusters. Let&#39;s check the results of the (penalized) membership entropy minimization: . fig, axes = plt.subplots(1, 1, figsize=(5, 4)) axes.plot([optimal_nclusters, optimal_nclusters], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) for i in range(0, total_entropies.shape[0]): axes.plot(np.arange(2, max_clusters, 1), total_entropies[i,:], color=(0.46,0.46,0.46,0.2), linewidth=2) axes.set_xlabel(&#39;Number of clusters&#39;) axes.set_ylabel(&#39;Total membership entropy&#39;) plt.tight_layout() plt.show() . . Here, the different lines correspond to different seeds. We can see the huge variability in the results, especially when testing a higher number of clusters. Not performing this ensemble analysis would lead to very far-from-optimal results. For the sake of completeness, let&#39;s run the optimal clustering and plot the results: . centers, p, representative_cluster, Sx, S = run_cluster(optimal_nclusters, dataset, seed=optimal_seed) print(&quot;Total membership entropy =&quot;, np.round(S, 2)) . Total membership entropy = 28.85 . def make_rgb_transparent(rgb, alpha): bg_rgb = [1, 1, 1] return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)] colormap = cm.get_cmap(&#39;Accent&#39;) edgecolors = list() facecolors = list() for i in range(0, optimal_nclusters): edgecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=1)) facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=0.65)) ### Plot fig, axes = plt.subplots(1, 1, figsize=(5, 4)) color_seq = list() for j in range(0, dataset.shape[0]): color_seq.append(make_rgb_transparent(edgecolors[representative_cluster[j]], 1-Sx[j])) for i in range(0, optimal_nclusters): axes.scatter([], [], label=str(i), color=edgecolors[i]) axes.scatter(dataset[:,0], dataset[:,1], marker=&#39;.&#39;, s=60, edgecolors=(0.6,0.6,0.6,0.5), c=color_seq) axes.scatter(centers[:,0], centers[:,1], color=(0.8,0.2,0.2, 0.8), marker=&quot;v&quot;, label=&quot;Cluster center&quot;) axes.set_xlabel(&#39;X&#39;) axes.set_ylabel(&#39;Y&#39;) axes.set_xlim(-1.2,1.2) axes.set_ylim(-1.2,1.2) axes.legend(loc=&quot;best&quot;) plt.tight_layout() plt.show() . Revisiting the concept of membership entropy regularization . As described above, promoting a lower number of clusters can be important when dealing with small datasets and/or a large number of clusters. Let&#39;s illustrate this. We consider the following dataset: . n=50 k=10 partition_seed=40 std=0.05 dataset_seed=41 partition = construct_random_partition(n=n, k=k, m=2, seed=partition_seed) print(&quot;Partition:&quot;, partition) dataset, _ = generate_random_dataset(partition=partition, n_features=2, std=std, seed=dataset_seed) . Partition: [ 4 3 2 12 7 3 2 4 7 6] . Let&#39;s begin by constructing unpenalized ($ lambda=0$) optimal clusters: . max_clusters=12 lamb=0 n_seeds=200 ### Membership entropy minimization max_clusters = 12 optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset, max_clusters=max_clusters, lamb=lamb, n_seeds=n_seeds) print(&quot;Optimal number of clusters =&quot;, optimal_nclusters) ### Clustering centers, p, representative_cluster, Sx, S = run_cluster(optimal_nclusters, dataset, seed=optimal_seed) print(&quot;Total membership entropy =&quot;, np.round(S, 2)) ### Plot edgecolors = list() facecolors = list() for i in range(0, optimal_nclusters): edgecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=1)) facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=0.65)) fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].plot([optimal_nclusters, optimal_nclusters], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) for i in range(0, total_entropies.shape[0]): axes[0].plot(np.arange(2, max_clusters, 1), total_entropies[i,:], color=(0.46,0.46,0.46,0.2), linewidth=2) axes[0].set_xlabel(&#39;Number of clusters&#39;) axes[0].set_ylabel(&#39;Total membership entropy&#39;) color_seq = list() for j in range(0, dataset.shape[0]): color_seq.append(make_rgb_transparent(edgecolors[representative_cluster[j]], 1-Sx[j])) for i in range(0, optimal_nclusters): axes[1].scatter([], [], label=str(i), color=edgecolors[i]) axes[1].scatter(dataset[:,0], dataset[:,1], marker=&#39;.&#39;, s=60, edgecolors=(0.6,0.6,0.6,0.5), c=color_seq) axes[1].scatter(centers[:,0], centers[:,1], color=(0.8,0.2,0.2, 0.8), marker=&quot;v&quot;, label=&quot;Cluster center&quot;) axes[1].set_xlabel(&#39;X&#39;) axes[1].set_ylabel(&#39;Y&#39;) axes[1].set_xlim(-1.2,1.2+0.5) axes[1].set_ylim(-1.2,1.2) axes[1].legend(loc=&quot;best&quot;) plt.tight_layout() plt.show() . . Optimal number of clusters = 9 Total membership entropy = 14.79 . Let&#39;s now promote a lower number of clusters by setting $ lambda=1$: . max_clusters=12 lamb=1 n_seeds=200 ### Membership entropy minimization max_clusters = 12 optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset, max_clusters=max_clusters, lamb=lamb, n_seeds=n_seeds) print(&quot;Optimal number of clusters =&quot;, optimal_nclusters) ### Clustering centers, p, representative_cluster, Sx, S = run_cluster(optimal_nclusters, dataset, seed=optimal_seed) print(&quot;Total membership entropy =&quot;, np.round(S, 2)) ### Plot edgecolors = list() facecolors = list() for i in range(0, optimal_nclusters): edgecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=1)) facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(optimal_nclusters-1)), alpha=0.65)) fig, axes = plt.subplots(1, 2, figsize=(10, 4)) axes[0].plot([optimal_nclusters, optimal_nclusters], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) for i in range(0, total_entropies.shape[0]): axes[0].plot(np.arange(2, max_clusters, 1), total_entropies[i,:], color=(0.46,0.46,0.46,0.2), linewidth=2) axes[0].set_xlabel(&#39;Number of clusters&#39;) axes[0].set_ylabel(&#39;Total membership entropy&#39;) color_seq = list() for j in range(0, dataset.shape[0]): color_seq.append(make_rgb_transparent(edgecolors[representative_cluster[j]], 1-Sx[j])) for i in range(0, optimal_nclusters): axes[1].scatter([], [], label=str(i), color=edgecolors[i]) axes[1].scatter(dataset[:,0], dataset[:,1], marker=&#39;.&#39;, s=60, edgecolors=(0.6,0.6,0.6,0.5), c=color_seq) axes[1].scatter(centers[:,0], centers[:,1], color=(0.8,0.2,0.2, 0.8), marker=&quot;v&quot;, label=&quot;Cluster center&quot;) axes[1].set_xlabel(&#39;X&#39;) axes[1].set_ylabel(&#39;Y&#39;) axes[1].set_xlim(-1.2,1.2+0.5) axes[1].set_ylim(-1.2,1.2) axes[1].legend(loc=&quot;best&quot;) plt.tight_layout() plt.show() . . Optimal number of clusters = 6 Total membership entropy = 17.37 . The unpenalized clustering recovers a number of clusters very close to the one used to sample the partition. However, given the combination of the small number of observations, large number of clusters and high proximity between the clusters, the penalized clustering may be considered, on some levels, superior to its unpenalized counterpart, despite its larger entropy. . Performance assessment . Let us now assess the performance of the optimal probabilistic clustering framework in a more systematic and comprehensive manner. Let&#39;s consider random datasets with different numbers of clusters k_vals and cluster widths std_vals. Let&#39;s however, keep the total number of observations n constant. For each combination we run the experiment n_realizations times. Since we will be only interested in recovering the correct number of clusters, we set $ lambda=0$: . k_vals = np.arange(3, 10, 1) std_vals = np.arange(0.0025, 0.03, 0.0025) n_realizations = 25 n = 500 lamb = 0 n_seeds = 100 max_clusters = 12 . The following function wraps all the procedures described above: . def run_experiment(n, # Number of observations k, # Number of clusters m, # Minimum number of observation per cluster seed_partition, # Seed to sample partition n_features, # Number of features (dimensionality of data space) std, # Width of clusters seed_dataset, # Seed to sample observations max_clusters, # Maximum number of clusters to try lamb, # Regularization parameters n_seeds): # Number of seeds to try # Samples partition partition = construct_random_partition(n=n, k=k, m=m, seed=seed_partition) # Samples dataset dataset, _ = generate_random_dataset(partition=partition, n_features=n_features, std=std, seed=seed_dataset) # Minimizes total membership entropy optimal_seed, optimal_nclusters, total_entropies = minimize_membership_entropy(dataset, max_clusters=max_clusters, lamb=lamb, n_seeds=n_seeds) return optimal_seed, optimal_nclusters . Let&#39;s run the experiment (the following code may take a long time to run): . experimental_results = np.zeros((n_realizations, len(k_vals), len(std_vals))) rand = np.random.RandomState(seed=41) for i in range(0, n_realizations): print(&quot;Running realization -&gt;&quot;, i) seed_partition = rand.randint(low=1, high=1e9, size=1) seed_dataset = rand.randint(low=1, high=1e9, size=1) for j, k in enumerate(k_vals): for h, std in enumerate(std_vals): _, optimal_nclusters = run_experiment(n=n, k=k, m=2, seed_partition=seed_partition, n_features=2, std=std, seed_dataset=seed_dataset, max_clusters=max_clusters, lamb=lamb, n_seeds=n_seeds) experimental_results[i,j,h] = optimal_nclusters . We can now quantify the performance by calculating the relative error in estimating the number of clusters, followed by its mean and standard deviation over the different realizations: . real_solution = np.repeat([k_vals], repeats=len(std_vals), axis=0).T deviation_mean = np.mean((experimental_results-real_solution)/real_solution, 0) deviation_std = np.std((experimental_results-real_solution)/real_solution, 0) . And finally plotting the results: . k_vals_plot = list(k_vals) k_vals_plot.append(k_vals[-1]+(k_vals[-1]-k_vals[-2])) std_vals_plot = list(std_vals) std_vals_plot.append(std_vals_plot[-1]+(std_vals_plot[-1]-std_vals_plot[-2])) fig = plt.figure(figsize=(10, 4)) axes1 = plt.axes([0.05, 0.05, 0.40, 0.80]) aux1=axes1.pcolormesh(std_vals_plot, k_vals_plot, deviation_mean, cmap=&#39;RdGy&#39;, vmin=-0.5, vmax=0.5) axes1.set_xlabel(&quot;Cluster width&quot;) axes1.set_ylabel(&quot;Number of clusters&quot;) cbar1 = fig.colorbar(aux1) cbar1.set_label(&#39;Average of relative errors&#39;, rotation=90) axes2 = plt.axes([0.55, 0.05, 0.40, 0.80]) aux2=axes2.pcolormesh(std_vals_plot, k_vals_plot, deviation_std, cmap=&#39;gist_earth&#39;) axes2.set_xlabel(&quot;Cluster width&quot;) axes2.set_ylabel(&quot;Number of clusters&quot;) cbar2 = fig.colorbar(aux2) cbar2.set_label(&#39;Standard deviation of relative errors&#39;, rotation=90) plt.show() . . As expected, the error in inferring the correct number of clusters increases when dealing with larger clusters. When increasing the number of clusters, however, the performance remains essentially constant, at least in the range investigated here. We also note a tendency to underestimate the number of clusters. This is not surprising. This apparent decrease in performance is not so much a consequence of a particular limitation in the framework, but rather due to the increasing probability of having cluster overlap when sampling different datasets. . In a future post, I will use the current framework to perform feature clustering, where the observations in the dataspace are features themselves. I will make use of the idea of feature distance, which I describe in a different post. . References: . de Prado, M. M. L. (2020). Machine learning for asset managers. Cambridge University Press. | Bezdek, J. C. (2013). Pattern recognition with fuzzy objective function algorithms. Springer Science &amp; Business Media. | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/clustering/entropy/membership%20entropy/regularization/2021/03/09/optimal_probabilistic_clustering_part2.html",
            "relUrl": "/clustering/entropy/membership%20entropy/regularization/2021/03/09/optimal_probabilistic_clustering_part2.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Metrics for feature distance",
            "content": "Introduction . Several quantities exist to quantify the relationship between two variables. By far, correlation and covariance are the most often used despite their clear limitations, which are briefly described here. On the other hand, many of these limitations can be overcome by resorting to more fundamental information-theoric quantities, like mutual information. Regardless, while correlation/covariance (for linear relations) and mutual information (for non-linear relations) allow us to quantify the dependence between say, $X$ and $Y$, they do not immediately define a proper metric. . If however, a metric can be defined to calculate the distance between $X$ and $Y$, denoted generally by $d(X, Y)$, one can introduce an intuitive topology on a set of features, with important implications, like clustering of features, which I will address in a future article. In this short article, I will describe available metrics for defining a distance between features. . Metrics for feature distance . Based on (de Prado, 2020), let me briefly introduce three different metrics: . 1. Correlation distance (two-sided): $$ d_ rho (X, Y) = sqrt{(1- rho(X,Y))/2} $$ . Here, $ rho(X,Y)$ is the correlation coefficient between $X$ and $Y$. This definition has the property that, if $X$ and $Y$ are perfectly anti-correlated the distance between them is maximal. On the other hand, if $ rho(X,Y) = 1 implies d_ rho = 0$. . 2. Correlation distance (one-sided): $$ d_{ vert rho vert} = sqrt{1 - vert rho(X,Y) vert} $$ . Depending on the application, we may want the property of zero distance for both perfect correlation and anti-correlation. This alternative definition satisfies that, besides setting a maximum distance between $X$ and $Y$ when they are completely uncorrelated. . 3. Variation of information: $$d_I(X,Y) = 1 - frac{I(X,Y)}{S(X,Y}$$ . Both correlation-based metrics above share the same limitation. They are only well defined if the relationship between $X$ and $Y$ is linear or, equivalently, if they follow a bivariate normal distribution. I have discussed in a previous post how mutual information allow us to measure a broader class of relations. In this context, we can define above entropy-based distance metric called variation of information. Here, $S(X,Y)$ and $I(X,Y)$ are the joint entropy and mutual information between $X$ and $Y$, respectively. In this framework, the distance between $X$ and $Y$ vanishes if and only if they are independent. Check out my previous post for a discussion and proper definition of entropy and mutual information. I will skip this step here. . All three quantities defined above are true metrics, in the sense that they satisfy all the properties required by a metric: non-negativity, symmetry and triangle inequality. They then allow the construction of a topological structure on a set of features. Besides, all three metrics are normalized, such that $0 leq d leq 1$. . Let&#39;s define some functions to calculate these metrics: . 1. Correlation distance (two-sided): . def calculate_corr_distance_2side(X, Y): rho = np.corrcoef(np.array((X, Y)))[0,1] distance = np.sqrt((1-rho)/2) return distance . 2. Correlation distance (one-sided): . def calculate_corr_distance_1side(X, Y): rho = np.corrcoef(np.array((X, Y)))[0,1] distance = np.sqrt(1-np.abs(rho)) return distance . 3. Variation of information: . import numpy as np def calculate_entropy(X): # 1) Histograms the samples nbins = int(len(X)**(1/3)) p = np.histogram(X, bins=nbins, density=False)[0] p = p/np.sum(p)+1e-6 # 2) Calculates the entropy entropy = -np.sum(p*np.log2(p)) return entropy def calculate_joint_entropy(X, Y): # 1) Histograms the samples nbins = int(len(X)**(1/3)) p = np.histogram2d(X, Y, bins=nbins, density=False)[0] p = p/np.sum(p)+1e-6 # 2) Calculates the entropy entropy = -np.sum(p*np.log2(p)) return entropy def calculate_mutual_information(X, Y): S_X = calculate_entropy(X) S_Y = calculate_entropy(Y) S_XY = calculate_joint_entropy(X, Y) I = S_X+S_Y-S_XY return I def calculate_variation_of_information(X, Y): I = calculate_mutual_information(X, Y) S = calculate_joint_entropy(X, Y) distance = 1 - I/S return distance . . Numerical illustration . Let me now generate some sample datasets to illustrate the behaviour of the different metrics defined above. . Linear relationships . I&#39;ll start by generating samples from a bivariate normal distribution for $(X,Y)$. We can construct correlated samples from uncorrelated ones by using the Cholesky decomposition, as implemented below: . from scipy.linalg import eigh, cholesky from scipy.stats import norm def generate_correlated_samples(N, rho): # The desired covariance matrix. r = np.array([[1.0, rho], [rho, 1.0]]) # Generate samples from 2 independent normally distributed N(0,1) x = norm.rvs(size=(2, N)) # Compute the Cholesky decomposition. c = cholesky(r, lower=True) # Convert the data to correlated random variables. y = np.dot(c, x) return (y[0,:], y[1,:]) . . Let&#39;s consider three cases of different correlation structure: . rhos = [0, 0.5, -0.95] samples = [generate_correlated_samples(N=1000, rho=rho) for rho in rhos] . Let&#39;s now plot the datasets, together with the corresponding distances between $X$ and $Y$: . import matplotlib import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 3, figsize=(14, 4)) for i, rho in enumerate(rhos): axes[i].plot(samples[i][0], samples[i][1], &#39;.&#39;, color=(0.5,0.5,0.8,0.5)) axes[i].set_xlabel(&quot;X&quot;) axes[i].set_ylabel(&quot;Y&quot;) axes[i].set_title(&quot;Correlation = &quot;+str(rho)+ &quot; n$d_ rho$ = &quot;+str(np.round(calculate_corr_distance_2side(samples[i][0], samples[i][1]),2))+ &quot;; $d_{ vert rho vert}$ = &quot;+str(np.round(calculate_corr_distance_1side(samples[i][0], samples[i][1]),2))+ &quot;; $d_I$ = &quot;+str(np.round(calculate_variation_of_information(samples[i][0], samples[i][1]),2))) plt.show() . . A few notes: . Case 1: As expected $d_{ vert rho vert}$ and $d_I$ estimate a distance close to 1 (maximum distance). Remember that, however, the two-sided correlation distance is defined such that $d_{ rho}=1$ when $X$ and $Y$ are perfectly uncorrelated. . | Case 2: All three metrics consider a smaller distance between $X$ and $Y$, consistent with the stronger correlation between them. . | Case 3: The two-side correlation distance is large, while the one-side correlation distance is small, because of the properties mentioned above. Variation of information is also smaller, because of the stronger relationship between $X$ and $Y$. . | . Finally, the different metrics seem to scale differently with respect to $ rho$. To better visualize this let&#39;s calculate all three metrics as we slowly increase the correlation coefficient from -1 to 1: . rhos = np.linspace(-1+1e-6, 1-1e-6, 200) samples = [generate_correlated_samples(N=5000, rho=rho) for rho in rhos] d_2side_rho = [calculate_corr_distance_2side(samples[i][0], samples[i][1]) for i in range(0, len(rhos))] d_1side_rho = [calculate_corr_distance_1side(samples[i][0], samples[i][1]) for i in range(0, len(rhos))] d_I = [calculate_variation_of_information(samples[i][0], samples[i][1]) for i in range(0, len(rhos))] . Plotting the results: . fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.plot(rhos, d_2side_rho, color=(0.8,0.3,0.3,0.5), label=&quot;Two-side correlation distance&quot;) axes.plot(rhos, d_1side_rho, color=(0.3,0.3,0.8,0.5), label=&quot;One-side correlation distance&quot;) axes.plot(rhos, d_I, color=(0.3,0.3,0.3,0.5), label=&quot;Variation of information&quot;) axes.set_xlabel(&quot;Correlation coefficient $ rho_{XY}$&quot;) axes.set_ylabel(&quot;Distance $(X, Y)$&quot;) axes.legend() plt.show() . . Nonlinear relationships . Let&#39;s now consider a nonlinear relationship between $X$ and $Y$ in the form $Y=X^2 + epsilon$, and generate a few samples of increasing amplitude in the noise term $ epsilon$: . noise = [0.1, 0.5, 2.0] X = norm.rvs(size=(1, 1000))[0,:] samples = list() for value in noise: Y = X**2 + value*norm.rvs(size=(1, 1000))[0,:] Y = (Y-np.mean(Y))/np.std(Y) samples.append([X,Y]) . And now plotting the results: . fig, axes = plt.subplots(1, 3, figsize=(14, 4)) for i, rho in enumerate(noise): axes[i].plot(samples[i][0], samples[i][1], &#39;.&#39;, color=(0.5,0.5,0.8,0.5)) axes[i].set_xlabel(&quot;X&quot;) axes[i].set_ylabel(&quot;Y&quot;) axes[i].set_title(&quot;$d_ rho$ = &quot;+str(np.round(calculate_corr_distance_2side(samples[i][0], samples[i][1]),2))+ &quot;; $d_{ vert rho vert}$ = &quot;+str(np.round(calculate_corr_distance_1side(samples[i][0], samples[i][1]),2))+ &quot;; $d_I$ = &quot;+str(np.round(calculate_variation_of_information(samples[i][0], samples[i][1]),2))) plt.show() . . As expected, correlation-based metrics fail in recognizing the degree of dependence between $X$ and $Y$ while variation of information does not. Note that, variation of information is still relatively high, even in the left case of low noise. This is partially because of the degeneracy in the variables. Knowing $Y$ does not uniquely determine $X$, even in the absence of noise. . Let&#39;s better visualize how all three metrics scale as we slowly increase the amplitude of the noise term. We begin by generating the samples: . noise = np.linspace(0, 2, 200) X = norm.rvs(size=(1, 10000))[0,:] samples = list() for value in noise: Y = X**2 + value*norm.rvs(size=(1, 10000))[0,:] Y = (Y-np.mean(Y))/np.std(Y) samples.append([X,Y]) . Now we calculate the different metrics: . d_2side_rho = [calculate_corr_distance_2side(samples[i][0], samples[i][1]) for i in range(0, len(noise))] d_1side_rho = [calculate_corr_distance_1side(samples[i][0], samples[i][1]) for i in range(0, len(noise))] d_I = [calculate_variation_of_information(samples[i][0], samples[i][1]) for i in range(0, len(noise))] . And finally plot the results: . fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.plot(noise, d_2side_rho, color=(0.8,0.3,0.3,0.5), label=&quot;Two-side correlation distance&quot;) axes.plot(noise, d_1side_rho, color=(0.3,0.3,0.8,0.5), label=&quot;One-side correlation distance&quot;) axes.plot(noise, d_I, color=(0.3,0.3,0.3,0.5), label=&quot;Variation of information&quot;) axes.set_xlabel(&quot;Amplitude of the noise term $ epsilon$&quot;) axes.set_ylabel(&quot;Distance $(X, Y)$&quot;) axes.legend() plt.show() . . Clearly, because of the non-linear relationship between $X$ and $Y$, both correlation-based metric completely fail in describing their decreasing inter-dependence. . References: . de Prado, M. M. L. (2020). Machine learning for asset managers. Cambridge University Press. | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/distance%20metrics/variation%20of%20information/correlation%20distance/2021/02/21/distance_metrics.html",
            "relUrl": "/distance%20metrics/variation%20of%20information/correlation%20distance/2021/02/21/distance_metrics.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Optimal probabilistic clustering - Part I",
            "content": "Introduction . In this series of articles, I will address the problem of clustering from a generic point of view, beyond the scope of finance and trading applications. In later articles, I will apply the ideas introduced here to problems arising during the systematic investment process. . I will use a probabilistic (fuzzy) clustering framework to develop some of the ideas. Here, instead of assigning each point in the feature space to a definite cluster, a set of membership probabilities $p_x(i)$ are calculated, describing the degree of confidence in assigning point $x$ to cluster $i$. . As with many other clustering techniques, a fundamental problem is to infer the optimal number of clusters, which enters as a free parameter in many of the respective algorithms. I will show that as a by-product of the probabilistic nature of the fuzzy clustering framework, we can define information-theoretic measures about the quality of the clustering and use an entropy minimization argument to infer the optimal number of clusters. I have originally developed these ideas when I was at Imperial College London, in the context of data-driven detection of phases and phase transitions in physical systems made up of only a few particles (Rodrigues et al., 2021). . Optimal clustering . As mentioned before, within the framework of fuzzy clustering, $p_x(i)$ is the probability of element $x$ belonging to cluster $i$, with $i=1,2,...,k$ and $k$ the total number of clusters. Here, I will estimate these probabilities using the fc-means algorithm (Bezdek, 2013), implemented in the Python package fcmeans (Dias, 2019). The ideas described here are, however, generic to any probabilistic clustering framework. . From the probabilities $p_x(i)$, I will define the membership entropy . $$S_x( { p_x(i) }) = - frac{1}{ mathrm{log}(k)} displaystyle sum_{i=1}^k p_x(i) mathrm{log}(p_x(i)),$$ . which quantifies the ambiguity in assigning observation $x$ to the available clusters. The least ambiguous case corresponds to $p_x(j) = 1$ and $p_x(i neq j)=0$ for a given cluster $j$, resulsting in $S_x=0$. On the other hand, a maximally ambiguous configuration $p_x(i) = 1/k$, leads to a maximum membership entropy $S_x=1$. . The $1/ mathrm{log}(k)$ term normalizes the membership entropy, such that configurations with different numbers of clusters can be correctly compared. This will be needed later to infer the optimal number of clusters. . Lastly, we define the representative cluster as that that, for a given observation $x$, maximizes the subset of probabilities $p_x(i)$. It is thus the most representative cluster associated with $x$. . Let&#39;s conduct numerical experiments to demonstrate these ideas. . Fuzzy clustering and information-theoretic metrics . Let&#39;s define a function that calculates the membership probabilities $p_x(i)$ using the fcmeans package and calculates the . Representative clusters | Membership entropy of every observation | Total membership entropy (across all observations) | from fcmeans import FCM def cluster(n_clusters, features): # membership probabilities model = FCM(n_clusters=n_clusters).fit(features) p = model.u # representative cluster representative_cluster = np.argmax(p, 1) # membership entropy Sx = -np.sum(p*np.log(p), 1) / np.log(n_clusters) # total membership entropy (across the entire feature space) S = np.sum(Sx) return p, representative_cluster, Sx, S . . Simulated data . We now generate a sample dataset with 5 clusters of 500 observations in total. Let&#39;s generate 3 cases of increasing spread among the observations: . N = 500 n_clusters = 5 blob_centers = [[0,0], [-1.0,1.0], [-1.0, -1.0], [1.0,-1.0], [1.0,1.0]] blobs_std = [0.25, 0.35, 0.45] from sklearn.datasets import make_blobs X_all = list() y_all = list() for std in blobs_std: X, y = make_blobs(n_samples=N, centers=blob_centers, cluster_std=[std]*n_clusters, n_features=2, random_state=0) X_all.append(X) y_all.append(y) . . Let&#39;s plot the dataset: . import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.rc(&#39;text&#39;, usetex = True) font = {&#39;family&#39;: &#39;normal&#39;, &#39;size&#39;: 12} matplotlib.rc(&#39;font&#39;, **font) matplotlib.rcParams[&#39;figure.dpi&#39;]= 1000 fig, axes = plt.subplots(1, 3, figsize=(14, 4)) for i, ax in enumerate(axes): ax.scatter(X_all[i][:,0], X_all[i][:,1], marker=&#39;.&#39;, s=80, edgecolor=(0.6,0.6,0.6,0.5), facecolor=(0.4,0.4,0.4)) ax.set_ylabel(&#39;feature 1&#39;) ax.set_xlabel(&#39;feature 2&#39;) ax.set_xlim([-2.4, 2.4]) ax.set_ylim([-2.4, 2.4]) ax.set_xticks([-2, -1, 0, 1, 2]) ax.set_yticks([-2, -1, 0, 1, 2]) plt.show() . . Membership entropy and optimal number of clusters . As mentioned before, an optimal number of clusters can be inferred as that that minimizes the total membership entropy across our feature space. Let&#39;s see how this quantity varies as a function of the number of clusters, for all three cases above: . n_clusters_trials = np.arange(2, 10, 1) total_entropies_all = list() for X in X_all: total_entropies = list() for n in n_clusters_trials: _, _, _, total_entropy = cluster(n_clusters=n, features=X) total_entropies.append(total_entropy) total_entropies_all.append(total_entropies) . . And plotting the results: . fig, axes = plt.subplots(1, 3, figsize=(14, 4)) for i, ax in enumerate(axes): ax.plot([5, 5], [0, np.max(total_entropies_all)], color=(0.8,0.6,0.6), linewidth=2) ax.plot(n_clusters_trials, total_entropies_all[i], color=(0.46,0.46,0.46), linewidth=2) ax.set_xlabel(&#39;Number of clusters&#39;, ) ax.set_ylabel(&#39;Total membership entropy&#39;) ax.tick_params(axis=&quot;both&quot;) ax.tick_params(direction=&#39;in&#39;, bottom=True, top=True, left=True, right=True) ax.set_xlim([1.5, 9.5]) ax.set_xscale(&#39;linear&#39;) ax.set_yscale(&#39;linear&#39;) plt.show() . . Few observations about the results above: . The minimization of the membership entropy correctly predicts the number of clusters in case 1 and 2. | Case 3 is, according to the membership entropy, equally consistent with 4 and 5 clusters. Indeed, by looking at the previous plot, it is not at all possible to say if the central part is a different cluster and simply the overlap of the 4 outer clusters. | The overall membership entropy increases from left to right, quantifying the increasing overlap between clusters. | Final clustering . Now that we have inferred the optimal number of clusters to be 5, let&#39;s perform the final clustering, including collecting the representative cluster of each observation. . representative_cluster_all = list() entropies_all = list() for X in X_all: p, representative_cluster, Sx, S = cluster(n_clusters=5, features=X) representative_cluster_all.append(representative_cluster) entropies_all.append(Sx) . . The membership probabilities enclose full information about the clustering. They are however hard to visualize. We can create a lower-dimensional visualization of the results by assigning different colours to different representative clusters. Further, we can assign a transparency level proportional to the membership entropy of each observation, thus gauging a level of ambiguity in assigning it to the available clusters: . ### Color handles def make_rgb_transparent(rgb, alpha): bg_rgb = [1, 1, 1] return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)] colormap = matplotlib.cm.get_cmap(&#39;Accent&#39;) edgecolors = list() facecolors = list() for i in range(0, 5): edgecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(5-1)), alpha=1)) facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(5-1)), alpha=0.65)) ### Plotting fig, axes = plt.subplots(1, 3, figsize=(14, 4)) for i, ax in enumerate(axes): color_seq = list() for j in range(0, X_all[i].shape[0]): color_seq.append(make_rgb_transparent(edgecolors[representative_cluster_all[i][j]], 1-entropies_all[i][j])) ax.scatter(X_all[i][:,0], X_all[i][:,1], marker=&#39;.&#39;, s=40, edgecolors=(0.6,0.6,0.6,0.5), c=color_seq) ax.set_ylabel(&#39;feature 1&#39;) ax.set_xlabel(&#39;feature 2&#39;) ax.set_xlim([-2.4, 2.4]) ax.set_ylim([-2.4, 2.4]) ax.set_xticks([-2, -1, 0, 1, 2]) ax.set_yticks([-2, -1, 0, 1, 2]) . . As expectable, the membership entropy increases in the regions between clusters. . Cluster quality . Finally, we can quantify the quality of the different clusters by calculating the intra-cluster mean membership entropy. Let&#39;s generate a different dataset to demonstrate this: . N = 400 blob_centers = [[2, 0], [2*np.cos(2*np.pi/3), 2*np.sin(2*np.pi/3)], [2*np.cos(4*np.pi/3), 2*np.sin(4*np.pi/3)]] blobs_std = [0.1, 0.3, 0.6] X, y = make_blobs(n_samples=N, centers=blob_centers, cluster_std=blobs_std, n_features=2, random_state=0) . . The dataset is plotted in the figure below. . Again, inferring the optimal number of clusters by minimizing the membership entropy: . n_clusters_trials = np.arange(2, 10, 1) total_entropies = list() for n in n_clusters_trials: _, _, _, total_entropy = cluster(n_clusters=n, features=X) total_entropies.append(total_entropy) . . The optimal number of clusters is correctly inferred to be 3, shown below. Let&#39;s then calculate the final clusters: . p, representative_cluster, Sx, S = cluster(n_clusters=3, features=X) . Plotting all the results: . ### Color handles edgecolors = list() facecolors = list() for i in range(0, 3): edgecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(3-1)), alpha=1)) facecolors.append(make_rgb_transparent(rgb=colormap(1.0*i/(3-1)), alpha=0.65)) fig, axes = plt.subplots(1, 3, figsize=(14, 4)) ### Sample dataset axes[0].scatter(X[:,0], X[:,1], marker=&#39;.&#39;, s=80, edgecolor=(0.6,0.6,0.6,0.5), facecolor=(0.4,0.4,0.4)) axes[0].set_ylabel(&#39;feature 1&#39;) axes[0].set_xlabel(&#39;feature 2&#39;) ### Minimization of the total membership entropy axes[1].plot([3, 3], [0, np.max(total_entropies)], color=(0.8,0.6,0.6), linewidth=2) axes[1].plot(n_clusters_trials, total_entropies, color=(0.46,0.46,0.46), linewidth=2) axes[1].set_xlabel(&#39;Number of clusters&#39;, ) axes[1].set_ylabel(&#39;Total membership entropy&#39;) axes[1].tick_params(axis=&quot;both&quot;) axes[1].tick_params(direction=&#39;in&#39;, bottom=True, top=True, left=True, right=True) axes[1].set_xlim([1.5, 9.5]) axes[1].set_xscale(&#39;linear&#39;) axes[1].set_yscale(&#39;linear&#39;) ### Final clusters color_seq = list() for j in range(0, X.shape[0]): color_seq.append(make_rgb_transparent(edgecolors[representative_cluster[j]], 1-Sx[j])) axes[2].scatter(X[:,0], X[:,1], marker=&#39;.&#39;, s=40, edgecolors=(0.6,0.6,0.6,0.5), c=color_seq) axes[2].set_ylabel(&#39;feature 1&#39;) axes[2].set_xlabel(&#39;feature 2&#39;) plt.show() . . And finally, calculate the intra-cluster mean membership entropy: . Si = list() for clust in set(representative_cluster): probs = p[np.argmax(p, 1)==clust, :] entropy = -np.sum(probs*np.log(probs), 1) / np.log(probs.shape[1]) Si.append(np.mean(entropy)) _=[print(&quot;Mean membership entropy across cluster {0} = {1}&quot;.format(i, np.round(Si[i], 2))) for i in range(0, len(Si))] . Mean membership entropy across cluster 0 = 0.02 Mean membership entropy across cluster 1 = 0.13 Mean membership entropy across cluster 2 = 0.3 . Here, the more robust clusters (higher quality) are those with lower mean membership entropy. . Final notes . The fc-means algorithm involves minimizing a given loss function. This minimization is not convex and, there is the change of being stuck at a local minimum. In more complex datasets than those generated above, it may be crucial to use different seeds in the initialization of the algorithm and do ensemble statistics over the final results, for instance, in finding the optimal number of clusters. | While I have conducted these experiments starting from the fc-means algorithm, the ideas described here can be applied to any other clustering algorithm that performs fuzzy (probabilistic clustering), like Gaussian mixture models, for instance. | In Part I of this series, I began by introducing the concept of membership entropy and showed that its minimization across the entire feature space leads to a form of optimal clustering. This has been shown, however, for a &quot;well behaved&quot; and relatively simple dataset. . In Part II I will describe some improvements to the current framework, which will allow us to better tackle more complex and inhomogeneous datasets. I will also conduct some systematic experiments to access the performance of the optimal probabilistic clustering concept. . References: . Rodrigues, J. D., Dhar, H. S., Walker, B. T., Smith, J. M., Oulton, R. F., Mintert, F., &amp; Nyman, R. A. (2021). Learning the Fuzzy Phases of Small Photonic Condensates. Phys. Rev. Lett., 126(15), 150602. https://doi.org/10.1103/PhysRevLett.126.150602 | Bezdek, J. C. (2013). Pattern recognition with fuzzy objective function algorithms. Springer Science &amp; Business Media. | Dias, M. L. D. (2019). fuzzy-c-means: An implementation of Fuzzy C-means clustering algorithm. Zenodo. https://doi.org/10.5281/zenodo.3066222 | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/clustering/entropy/membership%20entropy/2021/02/12/optimal_probabilistic_clustering_part1.html",
            "relUrl": "/clustering/entropy/membership%20entropy/2021/02/12/optimal_probabilistic_clustering_part1.html",
            "date": " • Feb 12, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Trading with the Kelly criterion",
            "content": "Motivation . Forecasting the direction of the next price movement is only part of the problem of trading. A proper capital allocation to that forecast its equally significant, althought is importance is often underestimated. Here, I&#39;m considering the problem of capital allocation on a single strategy, and not that of optimal portfolio allocation when we are investing in more than one instrument, like the framework of Modern Portfolio Theory. . A remarkable insight into this subject has been given by (Kelly, 1956). A good introduction to the subject can be found in Wikipedia. Thorpe also has a very interesting technical review (Thorp, 2008) and (Cover, 1999), like Kelly in its original paper, describes the interesting connections with information theory. . Kelly addresses the problem of optimal capital allocation under a statistically favourable (in expectation) betting opportunity. Two important conclusions from his work are: . Even in the presence of a favourable bet, over-allocation of capital will lead to ruin in the long run with probability 1. | Knowing the probability of the possible outcomes allows us to optimally size our bet (or position) in the sense of maximizing the expected growth rate of our portfolio. . Note: While point 1 may seem counterintuitive at first, think that if your portfolio falls 50%, it has to grow 100% to come back to its initial value. | Mathematical formulation . Let&#39;s consider the following formulation of the Kelly criterion: . Let&#39;s define $R_t$ as the random variable describing the returns of our strategy (or returns of a price time-series) at time $t$. The portfolio value at time $t+1$ is then the random variable $P_{t+1}$ given by . $$ P_{t+1} = p_t (1+ f R_t) $$ . with $p_t$ the portfolio value at $t$ and $f$ the respective fraction allocated to the strategy. Note that, $f$ can be negative, meaning that we are going short on the strategy (or instrument). If we have leverage available, $ vert f vert$ can also be greater than 1. The portfolio value $P_{t+1}$ can also be written as . $$ P_{t+1} = e^{ Lambda}, $$ . assuming, without loss of generality, $p_t = 1$. Here, $ Lambda = mathrm{log}(1 + f R_t)$ is the random variable describing the portfolio growth rate. The Kelly criterion gives the optimal value of $f$ in the sense of maximizing the expected value of $ Lambda$. If $R_t$ follows a normal distribution $ mathcal{N} ( mu_r, sigma_r)$, (Thorp, 2008) has shown that the Kelly-optimal value of $f$ is given by . $$ f = frac{ mu_r}{ sigma_r^2}. $$ . Note: In order to study the long-term portfolio growth, we are actually interested in the random variable . $$ P_{t+1} = p_1 (1+ f R_1)(1+ f R_2) ... (1+ f R_t). $$ . However, we are assuming that the $R_1$, $R_2$, ..., are independent. In this case, we can simplify the problem into the formulation above. . Monte-Carlo simulation . In order to appreciate the power of the Kelly criterion, we are going to conduct Monte-Carlo simulations, where at each time step the returns of our price time series are going to be drawn from a normal distribution $ mathcal{N} ( mu_r, sigma_r)$. However, in order to simulate a more realistic and dynamical scenario, $ mu_r$ and $ sigma_r$ will themselves be modelled as stochastic processes as well. In particular, I am going to model $ mu_{r,t}$ via an Ornstein-Ulhenbeck process, and $ sigma_{r,t}$ via a geometric Ornstein-Ulhenbeck process. The latter, for instance, allows the time-series to become heteroskedastic. . The Ornstein-Ulhenbeck process is defined as: . $$ dY_t = - frac{(Y_t - mu)}{ tau}dt + sigma sqrt{ frac{2}{ tau}} dW_t, $$ . with $ mu$ and $ sigma$ the process mean and standard deviation, respectively, and $W_t$ is Brownian motion. The geometric version is defined as . $$ dY_t = - frac{(Y_t - mu)}{ tau}dt + sigma Y_t dW_t. $$ . We can numerically integrate the stochastic differential equations above using the Euler-Maruyama method, implemented in the functions below: . Note: The entire code shown in this article has been written for the purpose of clarity rather than efficiency . import numpy as np def Ornstein_Ulhenbeck(T, dt, mu, sigma, tau, Y0): # Initializations Y = list() t = np.arange(0, T, dt) Y.append(Y0) # Parameters N = len(t) sigma_term = sigma*np.sqrt(2.0/tau)*np.sqrt(dt) normal_draws = np.random.normal(loc=0.0, scale=1.0, size=N) # Integration for i in range(1, N): Y.append(Y[-1] - dt*(Y[-1]-mu)/tau + sigma_term*normal_draws[i]) return np.array(Y) def geometric_Ornstein_Ulhenbeck(T, dt, mu, sigma, tau, Y0): # Initializations Y = list() t = np.arange(0, T, dt) Y.append(Y0) # Parameters N = len(t) sigma_term = sigma*np.sqrt(dt) normal_draws = np.random.normal(loc=0.0, scale=1.0, size=N) # Integration for i in range(1, N): Y.append(Y[-1] - dt*(Y[-1]-mu)/tau + sigma_term*Y[-1]*normal_draws[i]) return np.array(Y) . . Let&#39;s simulate 1000 price bars: . Stochastic path for the mean of the returns - $ mu_{r,t}$ . T = 1000 dt = 1 mu = 0 sigma = 0.002 tau = 1 path_mean = Ornstein_Ulhenbeck(T=T, dt=dt, mu=mu, sigma=sigma, tau=tau, Y0=mu) . And plotting the results: . import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 1, figsize=(14, 4)) axes.plot(path_mean, &#39;-&#39;, color=(0.8,0.5,0.5,1.0)) axes.set_xlabel(&quot;Bar&quot;) _=axes.set_ylabel(&quot;Path for returns mean&quot;) . . Stochastic path for the volatility of the returns - $ sigma_{r,t}$ . T = 1000 dt = 1 mu = 0.02 sigma = 0.01 tau = 10 path_std = geometric_Ornstein_Ulhenbeck(T=T, dt=dt, mu=mu, sigma=sigma, tau=tau, Y0=mu) . Plotting the results: . import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 1, figsize=(14, 4)) axes.plot(path_std, &#39;-&#39;, color=(0.8,0.5,0.5,1.0)) axes.set_xlabel(&quot;Bar&quot;) _=axes.set_ylabel(&quot;Path for returns volatility&quot;) . . Price path . From the paths generated above, we can now draw the returns $R_t sim mathcal{N} ( mu_{r,t}, sigma_{r,t})$ and the resulting price series $p_t = p_1 displaystyle prod_{t&#39;= 1}^t (1+r_{t&#39;})$: . path_returns = np.random.normal(loc=path_mean, scale=path_std) path_price = 100*np.cumprod(1+path_returns) . fig, axes = plt.subplots(1, 2, figsize=(14, 4)) axes[0].plot(path_returns, &#39;-&#39;, color=(0.8,0.5,0.5,1.0)) axes[0].set_xlabel(&quot;Bar&quot;) axes[0].set_ylabel(&quot;Realized returns&quot;) axes[0].set_title(&quot;Returns&quot;) axes[1].plot(path_price, &#39;-&#39;, color=(0.5,0.5,0.8,1.0)) axes[1].set_xlabel(&quot;Bar&quot;) axes[1].set_ylabel(&quot;Realized price&quot;) axes[1].set_title(&quot;Price&quot;) plt.show() . . The parameters above have been chosen such that the hit ratio - fraction of bars where the expected return $ mu_{r,t}$ is of the same sign as the realized return $r_t$ - varies approximately between 0.5 and 0.6. For this particular realization: . hit_ratio = np.sum(np.sign(path_returns*path_mean)&gt;0)/len(path_returns) print(hit_ratio) . 0.516 . Allocation strategies . Let&#39;s create a function that calculates the portfolio growth. It takes as parameters the initial portfolio value, the fraction of the portfolio invested at every bar (negative for short positions and positive for long positions), and the realized price returns . I am also assuming that we are effectively bankrupted it at any point our porfolio drops below 1 percent of its initial value: . def calculate_portfolio_growth(portfolio_initial, fractions, price_returns): N = len(fractions) strategy_returns = fractions*price_returns portfolio_growth = portfolio_initial*np.cumprod(1+strategy_returns) if True in [value&lt;=1e-2*portfolio_initial for value in portfolio_growth]: bust_ind = np.min(np.where(portfolio_growth&lt;1e-2*portfolio_initial)[0]) portfolio_growth[bust_ind:] = 0 return portfolio_growth . We are going to assume maximum available leverage of 10 and three different allocation strategies: . Full leverage | Half leverage | Optimal Kelly allocation (limited to a maximum of 10x leverage) | portfolio_initial = 1 leverage = 10 ##### Strategy 1 - full leverage fractions1 = leverage * np.sign(path_mean) portfolio1 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions1, price_returns=path_returns) ##### Strategy 2 - half leverage fractions2 = 0.5*leverage * np.sign(path_mean) portfolio2 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions2, price_returns=path_returns) ##### Strategy 3 - Kelly fractions3 = path_mean/path_std**2 fractions3[fractions3&gt;leverage] = leverage fractions3[fractions3&lt;-leverage] = -leverage portfolio3 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions3, price_returns=path_returns) . Let&#39;s now plot the performance of the different allocation strategies: . fig, axes = plt.subplots(1, 1, figsize=(7, 5)) axes.plot(portfolio1, &#39;-&#39;, label=&quot;Full leverage&quot;, color=(0.8,0.5,0.5,1.0)) axes.plot(portfolio2, &#39;-&#39;, label=&quot;Half leverage&quot;, color=(0.5,0.5,0.8,1.0)) axes.plot(portfolio3, &#39;-&#39;, label=&quot;Kelly optimal&quot;, color=(0.5,0.7,0.7,1.0)) axes.legend() axes.set_xlabel(&quot;Bar&quot;) axes.set_ylabel(&quot;Portfolio value&quot;) axes.set_yscale(&quot;log&quot;) . . As stated before, which we can now empirically corroborate, one of the key insights of the Kelly criterion is that, even in the presence of a favorable betting (or trading) opportunity, overallocation will eventually lead to ruin. . In the realization above, the full leverage allocation does lead to ruin. The half leverage strategy has a positive return but underperforms the optimal Kelly allocation. . We can calculate the average absolute leverage in the Kelly allocation: . print(np.round(np.mean(np.abs(fractions3)), 2)) . 5.09 . Thus, despite the similar used leverage (on average), the Kelly strategy outperforms the constant 5x leverage strategy. . A better way to conduct this analysis is through the Sharp ratio, which can be shown to be maximized (in expectation) by the Kelly-optimal allocation. . def calculate_sharp_ratio(portfolio): N = len(portfolio) if True in [value==0 for value in portfolio]: bust_ind = np.min(np.where(portfolio==0)[0]) portfolio = portfolio[:bust_ind] returns = (portfolio[1:]-portfolio[0:-1])/portfolio[0:-1] sharp_ratio = np.sqrt(252)*np.mean(returns)/np.std(returns) return sharp_ratio . In the above, we are associating each bar to a trading day, and annualizing the Sharp ratio. The results: . print(&quot;Sharp ratio, full leverage =&quot;, np.round(calculate_sharp_ratio(portfolio1),2)) print(&quot;Sharp ratio, half leverage =&quot;, np.round(calculate_sharp_ratio(portfolio2),2)) print(&quot;Sharp ratio, Kelly optimal =&quot;, np.round(calculate_sharp_ratio(portfolio3),2)) . . Sharp ratio, full leverage = -0.84 Sharp ratio, half leverage = 1.02 Sharp ratio, Kelly optimal = 1.67 . Note that, an overall multiplicative factor in the used leverage does not change the Sharp ratio. The reason why the Sharp ratio may be different between the full- and the half-leverage strategies is because of the possibility of going bankrupt. . Ensemble statistics . While the results above corroborate our expectations about the Kelly allocation, their statistical significance is somewhat meaningless, because we have considered a single realization, albeit a long one with a total of 1000. . I am going to perform many realizations (200) of the experiment above and check the performance statistics. . Note: The processes simulated here are essentially ergodic, meaning that $m$ realizations of $N$ bars each, is equivalent to a single realization of $mN$ bars, in terms of their statistical properties. For the sake of simplicity, I&#39;m going to choose the former. . n_runs = 200 strategy1 = list() strategy2 = list() strategy3 = list() for i in range(0, n_runs): # Price path path_mean = Ornstein_Ulhenbeck(T=1000, dt=1, mu=0, sigma=0.002, tau=1, Y0=0) path_std = geometric_Ornstein_Ulhenbeck(T=1000, dt=1, mu=0.02, sigma=0.01, tau=10, Y0=0.02) path_returns = np.random.normal(loc=path_mean, scale=path_std) path_price = 100*np.cumprod(1+path_returns) # Full leverage fractions1 = leverage * np.sign(path_mean) portfolio1 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions1, price_returns=path_returns) strategy1.append(calculate_sharp_ratio(portfolio1)) # Half leverage fractions2 = 0.5 * leverage * np.sign(path_mean) portfolio2 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions2, price_returns=path_returns) strategy2.append(calculate_sharp_ratio(portfolio2)) # Kelly fractions3 = path_mean/path_std**2 fractions3[fractions3&gt;leverage] = leverage fractions3[fractions3&lt;-leverage] = -leverage portfolio3 = calculate_portfolio_growth(portfolio_initial=portfolio_initial, fractions=fractions3, price_returns=path_returns) strategy3.append(calculate_sharp_ratio(portfolio3)) sharp_ratio = np.array([strategy1, strategy2, strategy3]).T strategy_labels = [&quot;Full leverage&quot;, &quot;Half leverage&quot;, &quot;Kelly optimal&quot;] . . And now plotting the results: . fig, axes = plt.subplots(1, 1, figsize=(6, 4)) axes.boxplot(sharp_ratio) axes.set_xticklabels(strategy_labels, rotation=0) axes.set_ylabel(&quot;Sharp ratio&quot;) plt.tight_layout() plt.show() print(&quot;Mean Sharp ratio, full leverage =&quot;, np.round(np.mean(sharp_ratio[:,0]),2)) print(&quot;Mean Sharp ratio, half leverage =&quot;, np.round(np.mean(sharp_ratio[:,1]),2)) print(&quot;Mean Sharp ratio, Kelly optimal =&quot;, np.round(np.mean(sharp_ratio[:,2]),2)) . . Mean Sharp ratio, full leverage = 1.3 Mean Sharp ratio, half leverage = 1.75 Mean Sharp ratio, Kelly optimal = 2.16 . Now we have statistically significant results demonstrating the outperformance of the Kelly allocation strategy. The Sharp ratio of the full-leverage strategy is highly left-skewed, due to the many times the strategy goes bankrupt. . Note: Quantitatively, the results shown here are strongly dependent on the parameters chosen in the Monte-Carlo simulation. One may even wonder if I&#39;m not somehow overfitting these parameters in order to arrive at the expected result. However, running the experiment for different sets of parameters yields that the Kelly allocation is systematically the one that shows better performance. . Final notes . I would like to end this article with a few notes: . I&#39;m applying the Kelly criterion in a slightly different way than usual. We typically consider the statistics of the returns of a given strategy, or instrument, and then calculate the optimal allocation fraction, which does not vary from one bar to the other. Then the task is to continuously rebalance the portfolio as to maintain that constant allocation fraction. Here, I&#39;m assuming that I am in the presence of a fully probabilistic pricing model which, at each bar, outputs the full probability distribution of the next price return. Given the assumption of independence between these distributions, I&#39;m conjecturing, but not proving, that by calculating the optimal Kelly fraction at each bar I&#39;m still maximizing the long-run expected growth rate of my portfolio, in expectation. | I constructed the simulations above assuming the returns to be normally distributed. While this is not a terrible assumption, it may not be a good one depending on the situation. The advantage of normally-distributed returns is the close-form solution for the optimal Kelly fraction. For a generic distribution, however, we can perform Monte-Carlo simulations to infer the optimal Kelly fraction. | By construction, in the controlled experiment described here, I know exactly the probability distribution of returns. In a real application, while we can develop full probabilistic models in complete analogy with the ideas described here, one must consider the uncertainty about the model itself. This may mean reducing, for instance, using a fraction of the optimal Kelly fraction to reduce the risk of model uncertainty. | References: . Kelly, J. L. (1956). A new interpretation of information rate. The Bell System Technical Journal, 35(4), 917–926. https://doi.org/10.1002/j.1538-7305.1956.tb03809.x | Thorp, E. O. (2008). The Kelly criterion in blackjack sports betting, and the stock market. In Handbook of asset and liability management (pp. 385–428). Elsevier. | Cover, T. M. (1999). Elements of information theory. John Wiley &amp; Sons. | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/kelly%20criterion/monte%20carlo/2021/02/07/kelly_criterion_in_trading.html",
            "relUrl": "/kelly%20criterion/monte%20carlo/2021/02/07/kelly_criterion_in_trading.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Mutual information for feature selection",
            "content": "Motivation . In this article, I will explore information-theoretic quantities (mutual information) to perform feature selection for models on continuous variables, like a sequence of price returns, for instance. Mutual information offers many advantages, including: . Model-independent feature selection: Many feature selection techniques are inherent to a particular class of models. For instance, boruta, gini, or mean impurity decrease use tree-based models to rank the importance among a set of features. While there is an obvious value in selecting features whose predictive power is better leveraged by a particular class of models, this approach may lead to the drop of informative features if the end model is not of the same class as that used for feature selection. In this context, mutual information offers a complete model-agnostic approach to feature selection. . | Full distribution feature selection: If we are trying to model a continuous variable, most feature selection techniques end up selecting features that do better in predicting the mean of the target variable. If we are trying to forecast the return of a given price series, for instance, it is as important to predict its mean as it is to predict is uncertainty (volatility), which will allow a better sized allocation to that particular strategy or forecast. This means that we often may benefit from selecting features that carry information about different aspects of the distribution of our target, and not only its mean. Mutual information does exactly that. Even if we are working with classification models, which output buy and sell signals according to some rules, it is extremely important to have a way to model the confidence of these signals. For instance, (De Prado, 2018) describes using meta-labeling and the training of a surrogate model to predict the confidence on the buy and sell signals derived from the principal model. The best features for this surrogate model may be different than those of the first model. . | Two different kinds of relationships . Let&#39;s conduct a controlled experiment using two different scenarios: . Variable $X$ (cause) partially determines the mean of variable $Y$ (effect) | Variable $X$ (cause) partially determines the scale (standard deviation) of variable $Y$ (effect) | For the sake of simplicity, let&#39;s restrict ourselves to normal distributions here. . Let&#39;s define a function that generates these samples: . Note: The entire code shown in this article has been written for the purpose of clarity rather than efficiency . import numpy as np def normalize(sample): return (sample-np.mean(sample))/np.std(sample) def generate_samples(N, dependence_coef, which_param): ### Cause samples_cause = np.random.normal(loc=0.0, scale=1.0, size=N) ### Effect inherent_noise = np.random.normal(loc=0.5, scale=1.0, size=N) param = normalize((1-dependence_coef)*inherent_noise + dependence_coef*samples_cause) samples_effect = list() for i in range(0, N): if which_param == 1: # relationship on the mean mean = param[i] scale = 1.0 if which_param == 2: # relationship on the scale mean = np.random.normal(loc=np.random.normal(loc=0, scale=1.0)) scale = np.exp(param[i]) [samples_effect.append(np.random.normal(loc=mean, scale=scale))] return [normalize(samples_cause), normalize(samples_effect)] . . Note that we are allowing a varying level of &quot;causation&quot; between $X$ and $Y$, determined by dependence_coef which varies between 0 (no relation) and 1 (strong relation). Also, strong relation does not imply a perfect correlation between the variables. We are constantly drawing points from univariate normal distributions, where the parameters of the distribution of $Y$ - either mean or scale - depend on the value of $X$, at each observation. This ensures a more complex and non-trivial stochastic relation between the two random variables. . Let&#39;s generate these samples, assuming a dependence coefficient of 0.5: . N = 10000 dependence_coef = 0.5 # Relationship on the mean (cause_mean, effect_mean) = generate_samples(N=N, dependence_coef=dependence_coef, which_param=1) # Relationship on the scale (cause_scale, effect_scale) = generate_samples(N=N, dependence_coef=dependence_coef, which_param=2) . . And now let&#39;s plot the samples: . import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True) axes[0].plot(cause_mean, effect_mean, &#39;.&#39;, color=(0.8,0.5,0.5,0.2)) axes[0].set_xlabel(&quot;Cause - X&quot;) axes[0].set_ylabel(&quot;Effect - Y&quot;) axes[0].set_title(&quot;Relation on the mean&quot;) axes[1].plot(cause_scale, effect_scale, &#39;.&#39;, color=(0.8,0.5,0.5,0.2)) axes[1].set_xlabel(&quot;Cause - X&quot;) axes[1].set_ylabel(&quot;Effect - Y&quot;) axes[1].set_title(&quot;Relation on the scale&quot;) _=plt.suptitle(&quot;Dependence coefficient = &quot;+str(dependence_coef)) . . As visible from above, on the left-hand side, $X$ is a good feature to use in predicting the mean of $Y$, but completely unrelated to its variance, which is completely stochastic by construction. On the other hand, on the right-hand side, $X$ is completely unrelated to the mean of $Y$, but it&#39;s moderately related to its scale. . Let&#39;s check other dependence coefficients: . Note: The way I constructed the &quot;Relation on the mean&quot; above results in an essentially linear relationship between $X$ and $Y$, which could be quantified by their respective covariance. However, covariance and correlation fail when the relationship between the mean of $X$ and $Y$ is nonlinear, whereas mutual information does not. The &quot;Relation on the scale&quot; construction, however, goes beyond both linear and non-linear relationships between the means of $X$ and $Y$. Despite that, and as I will demonstrate now, mutual information remains able to quantify the strength of this dependence. . Entropy, Joint Entropy and Mutual Information . I&#39;m going to quickly review the definition of entropy, joint entropy, and mutual information. A more detailed, yet simple, introduction to the subject can be found on Wikipedia, for instance. For a more complete description, (Cover, 1999) is a great reference. . For a discrete random variable $X$, we can define the Shannon entropy as . $$S(X) = - displaystyle sum_{i=1}^N p(x_i) mathrm{log}_2 [ p(x_i) ], $$ . and the joint entropy of $X$ and $Y$ as . $$S(X,Y) = - displaystyle sum_{i=1}^{N_x} displaystyle sum_{j=1}^{N_y} p(x_i, y_j) mathrm{log}_2 [ p(x_i, y_j) ].$$ . Note that, if $X$ and $Y$ are independent, $S(X,Y) = S(X) + S(Y)$. The mutual information can then be defined as . $$I(X,Y) = S(X) + S(Y) - S(X,Y),$$ . thus measuring the &quot;amount&quot; of information that is shared between $X$ and $Y$. As such, if $X$ and $Y$ are independent, $I(X,Y)=0$. An equivalent definition of mutual information can be written in terms of the conditional entropy: . $$I(X,Y) = S(X) - S(X|Y) = S(Y) - S(Y|X).$$ . We can then understand the mutual information as the information that we get about $X$ by knowing only $Y$, or vice versa. . Note: The choice of logarithm of base 2 above is somehow arbitrary, changing only the units we use to measure the &quot;quantity&quot; of information. The base 2 choice allows for this quantification to be made in terms of number of bits. . Sample estimation . While the theoretical definitions are simple and straightforward, robustly estimating these quantities from sample data is often tricky, especially when dealing with continuous random variables. Given that we are using synthetic data here, we can produce as large samples as we want, getting good statistical convergence if so desired. We thus take the simplest approach possible of histogramming the observations and applying the above definitions directly. . Let&#39;s define some functions: . def calculate_entropy(X): # 1) Histograms the samples nbins = int(len(X)**(1/3)) p = np.histogram(X, bins=nbins, density=False)[0] p = p/np.sum(p)+1e-6 # 2) Calculates the entropy entropy = -np.sum(p*np.log2(p)) return entropy def calculate_joint_entropy(X, Y): # 1) Histograms the samples nbins = int(len(X)**(1/3)) p = np.histogram2d(X, Y, bins=nbins, density=False)[0] p = p/np.sum(p)+1e-6 # 2) Calculates the entropy entropy = -np.sum(p*np.log2(p)) return entropy def calculate_mutual_information(X, Y): S_X = calculate_entropy(X) S_Y = calculate_entropy(Y) S_XY = calculate_joint_entropy(X, Y) I = S_X+S_Y-S_XY return I . . Dealing with discretization scaling . From the point of view of estimating entropy and joint entropy, the discretization procedure above introduces a scaling that needs to be controlled for. . To deal with this issue, besides estimating the mutual information for the original sample $(x_i, y_i)$ - $I_{ mathrm{sample}}$ - we are going to estimate the mutual information on a number of datasets where the observations of one of the original variables, say $Y$, are randomly permutated - $I_{ mathrm{perm}}^j$. We can then defined the mutual information score . $$s_I = frac{I_{ mathrm{sample}} - mathrm{mean}(I_{ mathrm{perm}}^j)}{ mathrm{sd}(I_{ mathrm{perm}}^j)}.$$ . By doing this normalization, we are arriving at a quantity that is insensitive to scaling issues arising from the discretization of $X$ and $Y$. In essence, $s_I$ measures the confidence, in terms of the number of standard deviations, that the relation between $X$ and $Y$ is not random. . Some code: . def calculate_mutual_information_score(X, Y, n_perm): # Mutual information on original samples I = calculate_mutual_information(X=X, Y=Y) # Mutual information on randomly shuffled data I_perm = list() ind = np.arange(len(Y)) for i in range(0, n_perm): np.random.shuffle(ind) Y_shuffled = Y[ind] I_perm.append(calculate_mutual_information(X=X, Y=Y_shuffled)) # Calculates the mutual information score mi_score = (I-np.mean(I_perm))/np.std(I_perm) return mi_score . . Let&#39;s now conduct some estimations of the mutual information score. . Let&#39;s begin with a small dependence between $X$ and $Y$ (here, $X$ related with the mean of $Y$): . (cause, effect) = generate_samples(N=100000, dependence_coef=0.05, which_param=1) n_perm = 100 mi_score = calculate_mutual_information_score(X=cause, Y=effect, n_perm=n_perm) print(np.round(mi_score,1)) . . 0.9 . and a slighly stronger relation: . (cause, effect) = generate_samples(N=100000, dependence_coef=0.20, which_param=1) n_perm = 100 mi_score = calculate_mutual_information_score(X=cause, Y=effect, n_perm=n_perm) print(np.round(mi_score,1)) . . 67.9 . Let&#39;s do a more systematic experiment, by slowly increasing the dependence coefficient from 0 to 1, and looking at the mutual information score for the 2 cases we have been considering - mean and scale relation: . dependence_coefs = np.linspace(0, 1, 20) mi_scores_mean = list() mi_scores_scale = list() for coef in dependence_coefs: ### On mean samples = generate_samples(N=100000, dependence_coef=coef, which_param=1) samples_cause = normalize(samples[0]) samples_effect = normalize(samples[1]) mi_scores_mean.append(calculate_mutual_information_score(X=samples_cause, Y=samples_effect, n_perm=50)) ### On scale samples = generate_samples(N=100000, dependence_coef=coef, which_param=2) samples_cause = normalize(samples[0]) samples_effect = normalize(samples[1]) mi_scores_scale.append(calculate_mutual_information_score(X=samples_cause, Y=samples_effect, n_perm=50)) . . And now plotting the results: . fig, axes = plt.subplots(1, 2, figsize=(10, 3)) axes[0].plot(dependence_coefs, mi_scores_mean, &#39;-&#39;, color=(0.8,0.5,0.5,1.0)) axes[0].set_xlabel(&quot;Dependence coeficient&quot;) axes[0].set_ylabel(&quot;Mutual information score&quot;) axes[0].set_title(&quot;Relation on the mean&quot;) axes[1].plot(dependence_coefs, mi_scores_scale, &#39;-&#39;, color=(0.8,0.5,0.5,1.0)) axes[1].set_xlabel(&quot;Dependence coeficient&quot;) axes[1].set_ylabel(&quot;Mutual information score&quot;) axes[1].set_title(&quot;Relation on the scale&quot;) plt.show() . . We correctly infer the increasing strength of the relationship between $X$ and $Y$. Note that, we are arriving at such large values of the mutual information score (or equivalently, such high confidence about the non-randomness of the relationship) because of the large number of points in our samples (100000). If we re-run the experiment with 10000 points we obtain: . Dealing with sample fluctuations . In any statistical estimation problem, we are subject to the problem of sample fluctuations. Even if our data-generating process is stationary, which is the case here, different samples will lead to different mutual information scores. . The problem is more severe if the data generating process is not stationary, which is often the case in financial time series. In this case, regime or structural shifts can change the relation between variables over time. The most we are left to do is to investigate the non-uniformity of our sample using empirical techniques, like resampling, and analyze inter-sample variations. . While bootstrap and bagging, for instance, would be appropriate for the experiment we are conducting, is not appropriate in financial time series, because it does not maintain the time ordering of the observations. The premise here is that while regime shifts may occur, there is some level of persistence of a given regime. . With this in mind, we are going to consider sequential resampling: . def calculate_resample_inds(N, n_groups): inds = np.arange(N) resample_inds = np.reshape(inds[0:n_groups*int(np.floor(N/n_groups))], (-1, n_groups)) return resample_inds . . And let&#39;s generate the samples: . n_groups = 10 dependence_coefs = np.linspace(0, 1, 20) mi_scores_mean = list() mi_scores_scale = list() # Resampling indices inds = calculate_resample_inds(N=100000, n_groups=n_groups) for coef in dependence_coefs: ### On mean # Full sample samples = generate_samples(N=100000, dependence_coef=coef, which_param=1) samples_cause = normalize(samples[0]) samples_effect = normalize(samples[1]) # Resampling vals = [calculate_mutual_information_score(X=samples_cause[inds[:,i]], Y=samples_effect[inds[:,i]], n_perm=50) for i in range(0, n_groups)] mi_scores_mean.append(vals) ### On scale # Full sample samples = generate_samples(N=100000, dependence_coef=coef, which_param=2) samples_cause = normalize(samples[0]) samples_effect = normalize(samples[1]) # Resampling vals = [calculate_mutual_information_score(X=samples_cause[inds[:,i]], Y=samples_effect[inds[:,i]], n_perm=50) for i in range(0, n_groups)] mi_scores_scale.append(vals) mi_scores_mean = np.array(mi_scores_mean) mi_scores_scale = np.array(mi_scores_scale) . . And now plotting the results: . fig, axes = plt.subplots(1, 2, figsize=(12, 4)) axes[0].boxplot(mi_scores_mean.T) axes[0].set_xticklabels(np.round(dependence_coefs,2), rotation=90) axes[0].set_xlabel(&quot;Dependence coeficient&quot;) axes[0].set_ylabel(&quot;Mutual information score&quot;) axes[0].set_title(&quot;Relation on the mean&quot;) axes[1].boxplot(mi_scores_scale.T) axes[1].set_xticklabels(np.round(dependence_coefs,2), rotation=90) axes[1].set_xlabel(&quot;Dependence coeficient&quot;) axes[1].set_ylabel(&quot;Mutual information score&quot;) axes[1].set_title(&quot;Relation on the scale&quot;) plt.show() . . This resampling partially quantifies sample fluctuations and inhomogeneities. However, while in this conducted experiment we can generate samples of arbitrary size, in real datasets we are limited by the possible small number of available observations. In this case, excessive resampling, especially sequential resampling like we&#39;re doing here may lead to completely useless information. . Conclusions . The idea behind this exposition was to demonstrate the strengths of mutual-information based feature selection. One big advantage is the fact we can infer relationships that are not captured by other feature selection techniques. In the context of trading, it is tremendously important to be able to predict not only the direction of the next price movement but also the volatility or uncertainty in this forecast. This can be done, for example, by developing full probabilistic regression models or, in the context of classifications models, using the meta-labeling technique, as described in (De Prado, 2018). Mutual information-based feature selection can then be used to construct these models. . References: . De Prado, M. L. (2018). Advances in financial machine learning. John Wiley &amp; Sons. | Cover, T. M. (1999). Elements of information theory. John Wiley &amp; Sons. | .",
            "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/information%20theory/mutual%20information/feature%20selection/feature%20importance/2021/02/06/mutal_information_and_feature_selection.html",
            "relUrl": "/information%20theory/mutual%20information/feature%20selection/feature%20importance/2021/02/06/mutal_information_and_feature_selection.html",
            "date": " • Feb 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a data scientist and systematic trader on a personal account. . Brief resume: . 2021 - present: Data Scientist @ Argus Media, development of forecast models for commodity markets; . | 2019 - 2020: Research Associate @ Imperial College London, machine learning applications in physics; . | 2014 - 2018: PhD in Physics @ University of Lisbon, machine learning description of turbulence; . | . For more information check the links in the bottom of this page. .",
          "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joaodmrodrigues.github.io/elements-financial-machine-learning/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}